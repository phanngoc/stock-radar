"""
CÃ´ng cá»¥ phÃ¢n tÃ­ch dá»¯ liá»‡u nÃ¢ng cao

Cung cáº¥p chá»©c nÄƒng phÃ¢n tÃ­ch nÃ¢ng cao nhÆ° phÃ¢n tÃ­ch xu hÆ°á»›ng Ä‘á»™ nÃ³ng, so sÃ¡nh ná»n táº£ng, Ä‘á»“ng xuáº¥t hiá»‡n tá»« khÃ³a, phÃ¢n tÃ­ch cáº£m xÃºc, v.v.ã€‚
"""

import re
from collections import Counter, defaultdict
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from difflib import SequenceMatcher

from ..services.data_service import DataService
from ..utils.validators import (
    validate_platforms,
    validate_limit,
    validate_keyword,
    validate_top_n,
    validate_date_range
)
from ..utils.errors import MCPError, InvalidParameterError, DataNotFoundError


def calculate_news_weight(news_data: Dict, rank_threshold: int = 5) -> float:
    """
    TÃ­nh trá»ng sá»‘ tin tá»©c (dÃ¹ng Ä‘á»ƒ sáº¯p xáº¿p)

    Triá»ƒn khai thuáº­t toÃ¡n trá»ng sá»‘ dá»±a trÃªn main.py, xem xÃ©t tá»•ng há»£pï¼š
    - Trá»ng sá»‘ xáº¿p háº¡ng (60%)ï¼šXáº¿p háº¡ng tin tá»©c trong báº£ng xáº¿p háº¡ng
    - é¢‘láº§næƒé‡ (30%)ï¼štin tá»©cå‡ºç°cá»§aláº§næ•°
    - çƒ­åº¦æƒé‡ (10%)ï¼šé«˜æ’åå‡ºç°cá»§aæ¯”ä¾‹

    Args:
        news_data: tin tá»©cæ•°æ®å­—å…¸ï¼ŒåŒ…å« ranks vÃ  count å­—æ®µ
        rank_threshold: é«˜æ’åé˜ˆå€¼ï¼Œé»˜è®¤5

    Returns:
        æƒé‡phÃºtæ•°ï¼ˆ0-100ä¹‹é—´cá»§aæµ®ç‚¹æ•°ï¼‰
    """
    ranks = news_data.get("ranks", [])
    if not ranks:
        return 0.0

    count = news_data.get("count", len(ranks))

    # æƒé‡cáº¥u hÃ¬nhï¼ˆvá»›i config.yaml ä¿æŒmá»™tè‡´ï¼‰
    RANK_WEIGHT = 0.6
    FREQUENCY_WEIGHT = 0.3
    HOTNESS_WEIGHT = 0.1

    # 1. Trá»ng sá»‘ thá»© háº¡ngï¼šÎ£(11 - min(rank, 10)) / Sá»‘ láº§n xuáº¥t hiá»‡n
    rank_scores = []
    for rank in ranks:
        score = 11 - min(rank, 10)
        rank_scores.append(score)

    rank_weight = sum(rank_scores) / len(ranks) if ranks else 0

    # 2. Trá»ng sá»‘ táº§n suáº¥tï¼šmin(Sá»‘ láº§n xuáº¥t hiá»‡n, 10) Ã— 10
    frequency_weight = min(count, 10) * 10

    # 3. TÄƒng cÆ°á»ng Ä‘á»™ nÃ³ngï¼šSá»‘ láº§n xáº¿p háº¡ng cao / æ€»Sá»‘ láº§n xuáº¥t hiá»‡n Ã— 100
    high_rank_count = sum(1 for rank in ranks if rank <= rank_threshold)
    hotness_ratio = high_rank_count / len(ranks) if ranks else 0
    hotness_weight = hotness_ratio * 100

    # ç»¼åˆæƒé‡
    total_weight = (
        rank_weight * RANK_WEIGHT
        + frequency_weight * FREQUENCY_WEIGHT
        + hotness_weight * HOTNESS_WEIGHT
    )

    return total_weight


class AnalyticsTools:
    """CÃ´ng cá»¥ phÃ¢n tÃ­ch dá»¯ liá»‡u nÃ¢ng caoç±»"""

    def __init__(self, project_root: str = None):
        """
        åˆå§‹åŒ–phÃºtæå·¥å…·

        Args:
            project_root: é¡¹ç›®æ ¹ç›®å½•
        """
        self.data_service = DataService(project_root)

    def analyze_data_insights_unified(
        self,
        insight_type: str = "platform_compare",
        topic: Optional[str] = None,
        date_range: Optional[Dict[str, str]] = None,
        min_frequency: int = 3,
        top_n: int = 20
    ) -> Dict:
        """
        ç»Ÿmá»™tæ•°æ®æ´å¯ŸphÃºtæå·¥å…· - æ•´åˆå¤šç§æ•°æ®phÃºtæcháº¿ Ä‘á»™

        Args:
            insight_type: æ´å¯Ÿloáº¡iï¼ŒcÃ³ thá»ƒé€‰å€¼ï¼š
                - "platform_compare": å¹³å°Ä‘á»‘iæ¯”phÃºtæï¼ˆÄ‘á»‘iæ¯”khÃ´ngåŒå¹³å°Ä‘á»‘iè¯é¢˜cá»§aå…³æ³¨åº¦ï¼‰
                - "platform_activity": å¹³å°æ´»è·ƒåº¦ç»Ÿè®¡ï¼ˆç»Ÿè®¡å„å¹³å°å‘å¸ƒé¢‘ç‡vÃ æ´»è·ƒgiá»é—´ï¼‰
                - "keyword_cooccur": å…³é”®è¯tá»•ngç°phÃºtæï¼ˆphÃºtæå…³é”®è¯åŒgiá»å‡ºç°cá»§acháº¿ Ä‘á»™ï¼‰
            topic: è¯é¢˜å…³é”®è¯ï¼ˆcÃ³ thá»ƒé€‰ï¼Œplatform_comparecháº¿ Ä‘á»™é€‚ç”¨ï¼‰
            date_range: ngÃ yæœŸèŒƒå›´ï¼ŒÄ‘á»‹nh dáº¡ng: {"start": "YYYY-MM-DD", "end": "YYYY-MM-DD"}
            min_frequency: nháº¥tå°tá»•ngç°é¢‘láº§nï¼ˆkeyword_cooccurcháº¿ Ä‘á»™ï¼‰ï¼Œé»˜è®¤3
            top_n: è¿”å›TOP Nç»“æœï¼ˆkeyword_cooccurcháº¿ Ä‘á»™ï¼‰ï¼Œé»˜è®¤20

        Returns:
            æ•°æ®æ´å¯ŸphÃºtæç»“æœå­—å…¸

        Examples:
            - analyze_data_insights_unified(insight_type="platform_compare", topic="trÃ­ tuá»‡ nhÃ¢n táº¡o")
            - analyze_data_insights_unified(insight_type="platform_activity", date_range={...})
            - analyze_data_insights_unified(insight_type="keyword_cooccur", min_frequency=5)
        """
        try:
            # tham sá»‘éªŒè¯
            if insight_type not in ["platform_compare", "platform_activity", "keyword_cooccur"]:
                raise InvalidParameterError(
                    f"æ— æ•ˆcá»§aæ´å¯Ÿloáº¡i: {insight_type}",
                    suggestion="æ”¯æŒcá»§aloáº¡i: platform_compare, platform_activity, keyword_cooccur"
                )

            # æ ¹æ®æ´å¯Ÿlá»›på‹è°ƒç”¨ç›¸åº”phÆ°Æ¡ng thá»©c
            if insight_type == "platform_compare":
                return self.compare_platforms(
                    topic=topic,
                    date_range=date_range
                )
            elif insight_type == "platform_activity":
                return self.get_platform_activity_stats(
                    date_range=date_range
                )
            else:  # keyword_cooccur
                return self.analyze_keyword_cooccurrence(
                    min_frequency=min_frequency,
                    top_n=top_n
                )

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def analyze_topic_trend_unified(
        self,
        topic: str,
        analysis_type: str = "trend",
        date_range: Optional[Dict[str, str]] = None,
        granularity: str = "day",
        threshold: float = 3.0,
        time_window: int = 24,
        lookahead_hours: int = 6,
        confidence_threshold: float = 0.7
    ) -> Dict:
        """
        ç»Ÿmá»™tè¯é¢˜è¶‹åŠ¿phÃºtæå·¥å…· - æ•´åˆå¤šç§è¶‹åŠ¿phÃºtæcháº¿ Ä‘á»™

        Args:
            topic: è¯é¢˜å…³é”®è¯ï¼ˆå¿…éœ€ï¼‰
            analysis_type: phÃºtæloáº¡iï¼ŒcÃ³ thá»ƒé€‰å€¼ï¼š
                - "trend": çƒ­åº¦è¶‹åŠ¿phÃºtæï¼ˆè¿½è¸ªè¯é¢˜cá»§açƒ­åº¦å˜åŒ–ï¼‰
                - "lifecycle": ç”Ÿå‘½å‘¨æœŸphÃºtæï¼ˆtá»«å‡ºç°Ä‘áº¿næ¶ˆå¤±cá»§aå®Œæ•´å‘¨æœŸï¼‰
                - "viral": ngoáº¡i lá»‡çƒ­åº¦æ£€æµ‹ï¼ˆè¯†åˆ«çªç„¶çˆ†ç«cá»§aè¯é¢˜ï¼‰
                - "predict": è¯é¢˜é¢„æµ‹ï¼ˆé¢„æµ‹æœªÄ‘áº¿ncÃ³ thá»ƒèƒ½cá»§axu hÆ°á»›ng nÃ³ngï¼‰
            date_range: ngÃ yæœŸèŒƒå›´ï¼ˆtrendvÃ lifecyclecháº¿ Ä‘á»™ï¼‰ï¼ŒcÃ³ thá»ƒé€‰
                       - **Ä‘á»‹nh dáº¡ng**: {"start": "YYYY-MM-DD", "end": "YYYY-MM-DD"}
                       - **é»˜è®¤**: khÃ´ngæŒ‡å®šgiá»é»˜è®¤phÃºtænháº¥tè¿‘7å¤©
            granularity: giá»é—´ç²’åº¦ï¼ˆtrendcháº¿ Ä‘á»™ï¼‰ï¼Œé»˜è®¤"day"ï¼ˆhour/dayï¼‰
            threshold: çƒ­åº¦çªå¢å€æ•°é˜ˆå€¼ï¼ˆviralcháº¿ Ä‘á»™ï¼‰ï¼Œé»˜è®¤3.0
            time_window: æ£€æµ‹giá»é—´çª—å£å°giá»æ•°ï¼ˆviralcháº¿ Ä‘á»™ï¼‰ï¼Œé»˜è®¤24
            lookahead_hours: é¢„æµ‹æœªÄ‘áº¿nå°giá»æ•°ï¼ˆpredictcháº¿ Ä‘á»™ï¼‰ï¼Œé»˜è®¤6
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆpredictcháº¿ Ä‘á»™ï¼‰ï¼Œé»˜è®¤0.7

        Returns:
            è¶‹åŠ¿phÃºtæç»“æœå­—å…¸

        Examples (å‡è®¾hÃ´m naylÃ  2025-11-17):
            - ç”¨æˆ·ï¼š"phÃºtæAInháº¥tè¿‘7å¤©cá»§aè¶‹åŠ¿" â†’ analyze_topic_trend_unified(topic="trÃ­ tuá»‡ nhÃ¢n táº¡o", analysis_type="trend", date_range={"start": "2025-11-11", "end": "2025-11-17"})
            - ç”¨æˆ·ï¼š"xemxemç‰¹æ–¯æ‹‰æœ¬thÃ¡ngcá»§açƒ­åº¦" â†’ analyze_topic_trend_unified(topic="ç‰¹æ–¯æ‹‰", analysis_type="lifecycle", date_range={"start": "2025-11-01", "end": "2025-11-17"})
            - analyze_topic_trend_unified(topic="æ¯”ç‰¹å¸", analysis_type="viral", threshold=3.0)
            - analyze_topic_trend_unified(topic="ChatGPT", analysis_type="predict", lookahead_hours=6)
        """
        try:
            # tham sá»‘éªŒè¯
            topic = validate_keyword(topic)

            if analysis_type not in ["trend", "lifecycle", "viral", "predict"]:
                raise InvalidParameterError(
                    f"æ— æ•ˆcá»§aphÃºtæloáº¡i: {analysis_type}",
                    suggestion="æ”¯æŒcá»§aloáº¡i: trend, lifecycle, viral, predict"
                )

            # æ ¹æ®phÃ¢n tÃ­chlá»›på‹è°ƒç”¨ç›¸åº”phÆ°Æ¡ng thá»©c
            if analysis_type == "trend":
                return self.get_topic_trend_analysis(
                    topic=topic,
                    date_range=date_range,
                    granularity=granularity
                )
            elif analysis_type == "lifecycle":
                return self.analyze_topic_lifecycle(
                    topic=topic,
                    date_range=date_range
                )
            elif analysis_type == "viral":
                # viralCháº¿ Ä‘á»™khÃ´ngéœ€muá»‘ntopictham sá»‘ï¼ŒSá»­ dá»¥ng chungæ£€æµ‹
                return self.detect_viral_topics(
                    threshold=threshold,
                    time_window=time_window
                )
            else:  # predict
                # predictCháº¿ Ä‘á»™khÃ´ngéœ€muá»‘ntopictham sá»‘ï¼ŒSá»­ dá»¥ng chungé¢„æµ‹
                return self.predict_trending_topics(
                    lookahead_hours=lookahead_hours,
                    confidence_threshold=confidence_threshold
                )

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def get_topic_trend_analysis(
        self,
        topic: str,
        date_range: Optional[Dict[str, str]] = None,
        granularity: str = "day"
    ) -> Dict:
        """
        çƒ­åº¦è¶‹åŠ¿phÃºtæ - è¿½è¸ªç‰¹å®šè¯é¢˜cá»§açƒ­åº¦å˜åŒ–è¶‹åŠ¿

        Args:
            topic: è¯é¢˜å…³é”®è¯
            date_range: ngÃ yæœŸèŒƒå›´ï¼ˆcÃ³ thá»ƒé€‰ï¼‰
                       - **Ä‘á»‹nh dáº¡ng**: {"start": "YYYY-MM-DD", "end": "YYYY-MM-DD"}
                       - **é»˜è®¤**: khÃ´ngæŒ‡å®šgiá»é»˜è®¤phÃºtænháº¥tè¿‘7å¤©
            granularity: giá»é—´ç²’åº¦ï¼Œä»…æ”¯æŒ dayï¼ˆå¤©ï¼‰

        Returns:
            è¶‹åŠ¿phÃºtæç»“æœå­—å…¸

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "å¸®tÃ´iphÃºtæmá»™tdÆ°á»›i'trÃ­ tuá»‡ nhÃ¢n táº¡o'nÃ yä¸ªè¯é¢˜nháº¥tè¿‘má»™tå‘¨cá»§açƒ­åº¦è¶‹åŠ¿"
            - "æŸ¥xem'æ¯”ç‰¹å¸'è¿‡Ä‘imá»™tå‘¨cá»§açƒ­åº¦å˜åŒ–"
            - "xemxem'iPhone'nháº¥tè¿‘7å¤©cá»§aè¶‹åŠ¿å¦‚ä½•"
            - "phÃºtæ'ç‰¹æ–¯æ‹‰'nháº¥tè¿‘má»™tthÃ¡ngcá»§açƒ­åº¦è¶‹åŠ¿"
            - "æŸ¥xem'ChatGPT'2024nÄƒm12thÃ¡ngcá»§aè¶‹åŠ¿å˜åŒ–"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> tools = AnalyticsTools()
            >>> # phÃºtæ7å¤©è¶‹åŠ¿ï¼ˆå‡è®¾hÃ´m naylÃ  2025-11-17ï¼‰
            >>> result = tools.get_topic_trend_analysis(
            ...     topic="trÃ­ tuá»‡ nhÃ¢n táº¡o",
            ...     date_range={"start": "2025-11-11", "end": "2025-11-17"},
            ...     granularity="day"
            ... )
            >>> # phÃºtæå†å²thÃ¡ngä»½è¶‹åŠ¿
            >>> result = tools.get_topic_trend_analysis(
            ...     topic="ç‰¹æ–¯æ‹‰",
            ...     date_range={"start": "2024-12-01", "end": "2024-12-31"},
            ...     granularity="day"
            ... )
            >>> print(result['trend_data'])
        """
        try:
            # éªŒè¯tham sá»‘
            topic = validate_keyword(topic)

            # éªŒè¯ç²’åº¦tham sá»‘ï¼ˆåªæ”¯æŒdayï¼‰
            if granularity != "day":
                from ..utils.errors import InvalidParameterError
                raise InvalidParameterError(
                    f"khÃ´ngæ”¯æŒcá»§aç²’åº¦å‚æ•°: {granularity}",
                    suggestion="å½“å‰ä»…æ”¯æŒ 'day' ç²’åº¦ï¼ŒvÃ¬åº•å±‚æ•°æ®theoå¤©èšåˆ"
                )

            # Xá»­ lÃ½ngÃ yèŒƒå›´ï¼ˆkhÃ´ngæŒ‡å®šgiá»é»˜è®¤nháº¥tè¿‘7å¤©ï¼‰
            if date_range:
                from ..utils.validators import validate_date_range
                date_range_tuple = validate_date_range(date_range)
                start_date, end_date = date_range_tuple
            else:
                # é»˜è®¤nháº¥tè¿‘7å¤©
                end_date = datetime.now()
                start_date = end_date - timedelta(days=6)

            # æ”¶é›†è¶‹åŠ¿dá»¯ liá»‡u
            trend_data = []
            current_date = start_date

            while current_date <= end_date:
                try:
                    all_titles, _, _ = self.data_service.parser.read_all_titles_for_date(
                        date=current_date
                    )

                    # thá»‘ng kÃªè¯¥thá»i gianç‚¹cá»§aè¯é¢˜Sá»‘ láº§n xuáº¥t hiá»‡n
                    count = 0
                    matched_titles = []

                    for _, titles in all_titles.items():
                        for title in titles.keys():
                            if topic.lower() in title.lower():
                                count += 1
                                matched_titles.append(title)

                    trend_data.append({
                        "date": current_date.strftime("%Y-%m-%d"),
                        "count": count,
                        "sample_titles": matched_titles[:3]  # åªgiá»¯ láº¡iå‰3ä¸ªæ ·æœ¬
                    })

                except DataNotFoundError:
                    trend_data.append({
                        "date": current_date.strftime("%Y-%m-%d"),
                        "count": 0,
                        "sample_titles": []
                    })

                # theoå¤©å¢åŠ thá»i gian
                current_date += timedelta(days=1)

            # tÃ­nh toÃ¡nè¶‹åŠ¿æŒ‡æ ‡
            counts = [item["count"] for item in trend_data]
            total_days = (end_date - start_date).days + 1

            if len(counts) >= 2:
                # tÃ­nh toÃ¡næ¶¨è·Œå¹…åº¦
                first_non_zero = next((c for c in counts if c > 0), 0)
                last_count = counts[-1]

                if first_non_zero > 0:
                    change_rate = ((last_count - first_non_zero) / first_non_zero) * 100
                else:
                    change_rate = 0

                # æ‰¾Ä‘áº¿nå³°giÃ¡ trá»‹thá»i gian
                max_count = max(counts)
                peak_index = counts.index(max_count)
                peak_time = trend_data[peak_index]["date"]
            else:
                change_rate = 0
                peak_time = None
                max_count = 0

            return {
                "success": True,
                "topic": topic,
                "date_range": {
                    "start": start_date.strftime("%Y-%m-%d"),
                    "end": end_date.strftime("%Y-%m-%d"),
                    "total_days": total_days
                },
                "granularity": granularity,
                "trend_data": trend_data,
                "statistics": {
                    "total_mentions": sum(counts),
                    "average_mentions": round(sum(counts) / len(counts), 2) if counts else 0,
                    "peak_count": max_count,
                    "peak_time": peak_time,
                    "change_rate": round(change_rate, 2)
                },
                "trend_direction": "trÃªnå‡" if change_rate > 10 else "dÆ°á»›ié™" if change_rate < -10 else "ç¨³å®š"
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def compare_platforms(
        self,
        topic: Optional[str] = None,
        date_range: Optional[Dict[str, str]] = None
    ) -> Dict:
        """
        å¹³å°Ä‘á»‘iæ¯”phÃºtæ - Ä‘á»‘iæ¯”khÃ´ngåŒå¹³å°Ä‘á»‘iåŒmá»™tè¯é¢˜cá»§aå…³æ³¨åº¦

        Args:
            topic: è¯é¢˜å…³é”®è¯ï¼ˆcÃ³ thá»ƒé€‰ï¼ŒkhÃ´ngæŒ‡å®šåˆ™Ä‘á»‘iæ¯”æ•´ä½“æ´»è·ƒåº¦ï¼‰
            date_range: ngÃ yæœŸèŒƒå›´ï¼ŒÄ‘á»‹nh dáº¡ng: {"start": "YYYY-MM-DD", "end": "YYYY-MM-DD"}

        Returns:
            å¹³å°Ä‘á»‘iæ¯”phÃºtæç»“æœ

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "Ä‘á»‘iæ¯”má»™tdÆ°á»›iå„ä¸ªå¹³å°Ä‘á»‘i'trÃ­ tuá»‡ nhÃ¢n táº¡o'è¯é¢˜cá»§aå…³æ³¨åº¦"
            - "xemxemçŸ¥ä¹vÃ å¾®åšå“ªä¸ªå¹³å°hÆ¡nå…³æ³¨ç§‘æŠ€tin tá»©c"
            - "phÃºtæå„å¹³å°hÃ´m naycá»§axu hÆ°á»›ng nÃ³ngphÃºtå¸ƒ"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> # Ä‘á»‘iæ¯”å„å¹³å°ï¼ˆå‡è®¾hÃ´m naylÃ  2025-11-17ï¼‰
            >>> result = tools.compare_platforms(
            ...     topic="trÃ­ tuá»‡ nhÃ¢n táº¡o",
            ...     date_range={"start": "2025-11-08", "end": "2025-11-17"}
            ... )
            >>> print(result['platform_stats'])
        """
        try:
            # tham sá»‘éªŒè¯
            if topic:
                topic = validate_keyword(topic)
            date_range_tuple = validate_date_range(date_range)

            # ç¡®å®šngÃ yèŒƒå›´
            if date_range_tuple:
                start_date, end_date = date_range_tuple
            else:
                start_date = end_date = datetime.now()

            # æ”¶é›†å„å¹³å°dá»¯ liá»‡u
            platform_stats = defaultdict(lambda: {
                "total_news": 0,
                "topic_mentions": 0,
                "unique_titles": set(),
                "top_keywords": Counter()
            })

            # éå†ngÃ yèŒƒå›´
            current_date = start_date
            while current_date <= end_date:
                try:
                    all_titles, id_to_name, _ = self.data_service.parser.read_all_titles_for_date(
                        date=current_date
                    )

                    for platform_id, titles in all_titles.items():
                        platform_name = id_to_name.get(platform_id, platform_id)

                        for title in titles.keys():
                            platform_stats[platform_name]["total_news"] += 1
                            platform_stats[platform_name]["unique_titles"].add(title)

                            # náº¿uæŒ‡å®šrá»“iè¯é¢˜ï¼Œthá»‘ng kÃªåŒ…å«è¯é¢˜cá»§amá»›ié—»
                            if topic and topic.lower() in title.lower():
                                platform_stats[platform_name]["topic_mentions"] += 1

                            # TrÃ­ch xuáº¥tå…³é”®è¯ï¼ˆç®€å•phÃºtè¯ï¼‰
                            keywords = self._extract_keywords(title)
                            platform_stats[platform_name]["top_keywords"].update(keywords)

                except DataNotFoundError:
                    pass

                current_date += timedelta(days=1)

            # è½¬æ¢vÃ¬cÃ³ thá»ƒåºåˆ—åŒ–cá»§aÄ‘á»‹nh dáº¡ng
            result_stats = {}
            for platform, stats in platform_stats.items():
                coverage_rate = 0
                if stats["total_news"] > 0:
                    coverage_rate = (stats["topic_mentions"] / stats["total_news"]) * 100

                result_stats[platform] = {
                    "total_news": stats["total_news"],
                    "topic_mentions": stats["topic_mentions"],
                    "unique_titles": len(stats["unique_titles"]),
                    "coverage_rate": round(coverage_rate, 2),
                    "top_keywords": [
                        {"keyword": k, "count": v}
                        for k, v in stats["top_keywords"].most_common(5)
                    ]
                }

            # æ‰¾å‡ºå„å¹³å°ç‹¬cÃ³cá»§axu hÆ°á»›ng nÃ³ng
            unique_topics = self._find_unique_topics(platform_stats)

            return {
                "success": True,
                "topic": topic,
                "date_range": {
                    "start": start_date.strftime("%Y-%m-%d"),
                    "end": end_date.strftime("%Y-%m-%d")
                },
                "platform_stats": result_stats,
                "unique_topics": unique_topics,
                "total_platforms": len(result_stats)
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def analyze_keyword_cooccurrence(
        self,
        min_frequency: int = 3,
        top_n: int = 20
    ) -> Dict:
        """
        å…³é”®è¯tá»•ngç°phÃºtæ - phÃºtæå“ªäº›å…³é”®è¯ç»å¸¸åŒgiá»å‡ºç°

        Args:
            min_frequency: nháº¥tå°tá»•ngç°é¢‘láº§n
            top_n: è¿”å›TOP Nå…³é”®è¯Ä‘á»‘i

        Returns:
            å…³é”®è¯tá»•ngç°phÃºtæç»“æœ

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "phÃºtæmá»™tdÆ°á»›iå“ªäº›å…³é”®è¯ç»å¸¸má»™tèµ·å‡ºç°"
            - "xemxem'trÃ­ tuá»‡ nhÃ¢n táº¡o'ç»å¸¸vÃ å“ªäº›è¯má»™tèµ·å‡ºç°"
            - "æ‰¾å‡ºhÃ´m naytin tá»©ctrongcá»§aå…³é”®è¯å…³è”"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> tools = AnalyticsTools()
            >>> result = tools.analyze_keyword_cooccurrence(
            ...     min_frequency=5,
            ...     top_n=15
            ... )
            >>> print(result['cooccurrence_pairs'])
        """
        try:
            # tham sá»‘éªŒè¯
            min_frequency = validate_limit(min_frequency, default=3, max_limit=100)
            top_n = validate_top_n(top_n, default=20)

            # Ä‘á»chÃ´m naycá»§adá»¯ liá»‡u
            all_titles, _, _ = self.data_service.parser.read_all_titles_for_date()

            # å…³é”®è¯tá»•ngç°thá»‘ng kÃª
            cooccurrence = Counter()
            keyword_titles = defaultdict(list)

            for platform_id, titles in all_titles.items():
                for title in titles.keys():
                    # TrÃ­ch xuáº¥tå…³é”®è¯
                    keywords = self._extract_keywords(title)

                    # báº£n ghimá»—iå…³é”®è¯å‡ºç°cá»§aæ ‡é¢˜
                    for kw in keywords:
                        keyword_titles[kw].append(title)

                    # tÃ­nh toÃ¡nä¸¤ä¸¤tá»•ngç°
                    if len(keywords) >= 2:
                        for i, kw1 in enumerate(keywords):
                            for kw2 in keywords[i+1:]:
                                # ç»Ÿmá»™tsáº¯p xáº¿pï¼ŒtrÃ¡nhé‡å¤
                                pair = tuple(sorted([kw1, kw2]))
                                cooccurrence[pair] += 1

            # lá»cä½é¢‘tá»•ngç°
            filtered_pairs = [
                (pair, count) for pair, count in cooccurrence.items()
                if count >= min_frequency
            ]

            # sáº¯p xáº¿på¹¶å–TOP N
            top_pairs = sorted(filtered_pairs, key=lambda x: x[1], reverse=True)[:top_n]

            # æ„å»ºç»“æœ
            result_pairs = []
            for (kw1, kw2), count in top_pairs:
                # æ‰¾å‡ºåŒgiá»åŒ…å«ä¸¤ä¸ªå…³é”®è¯cá»§aæ ‡é¢˜æ ·æœ¬
                titles_with_both = [
                    title for title in keyword_titles[kw1]
                    if kw2 in self._extract_keywords(title)
                ]

                result_pairs.append({
                    "keyword1": kw1,
                    "keyword2": kw2,
                    "cooccurrence_count": count,
                    "sample_titles": titles_with_both[:3]
                })

            return {
                "success": True,
                "cooccurrence_pairs": result_pairs,
                "total_pairs": len(result_pairs),
                "min_frequency": min_frequency,
                "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def analyze_sentiment(
        self,
        topic: Optional[str] = None,
        platforms: Optional[List[str]] = None,
        date_range: Optional[Dict[str, str]] = None,
        limit: int = 50,
        sort_by_weight: bool = True,
        include_url: bool = False
    ) -> Dict:
        """
        æƒ…æ„Ÿå€¾å‘phÃºtæ - táº¡oç”¨á»Ÿ AI æƒ…æ„ŸphÃºtæcá»§aç»“æ„åŒ–æç¤ºè¯

        æœ¬å·¥å…·æ”¶é›†tin tá»©cæ•°æ®å¹¶táº¡oä¼˜åŒ–cá»§a AI æç¤ºè¯ï¼Œbáº¡ncÃ³ thá»ƒsáº½å…¶gá»­iç»™ AI è¿›è¡Œæ·±åº¦æƒ…æ„ŸphÃºtæã€‚

        Args:
            topic: è¯é¢˜å…³é”®è¯ï¼ˆcÃ³ thá»ƒé€‰ï¼‰ï¼ŒåªphÃºtæåŒ…å«è¯¥å…³é”®è¯cá»§atin tá»©c
            platforms: å¹³å°lá»cåˆ—è¡¨ï¼ˆcÃ³ thá»ƒé€‰ï¼‰ï¼Œå¦‚ ['zhihu', 'weibo']
            date_range: ngÃ yæœŸèŒƒå›´ï¼ˆcÃ³ thá»ƒé€‰ï¼‰ï¼ŒÄ‘á»‹nh dáº¡ng: {"start": "YYYY-MM-DD", "end": "YYYY-MM-DD"}
                       khÃ´ngæŒ‡å®šåˆ™é»˜è®¤æŸ¥è¯¢hÃ´m naycá»§aæ•°æ®
            limit: è¿”å›tin tá»©cæ•°é‡giá»›i háº¡nï¼Œé»˜è®¤50ï¼Œnháº¥tå¤§100
            sort_by_weight: lÃ å¦theoæƒé‡æ’åºï¼Œé»˜è®¤Trueï¼ˆæ¨èï¼‰
            include_url: lÃ å¦åŒ…å«URLé“¾æ¥ï¼Œé»˜è®¤Falseï¼ˆèŠ‚çœtokenï¼‰

        Returns:
            åŒ…å« AI æç¤ºè¯vÃ tin tá»©cæ•°æ®cá»§aç»“æ„åŒ–ç»“æœ

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "phÃºtæmá»™tdÆ°á»›ihÃ´m naytin tá»©ccá»§aæƒ…æ„Ÿå€¾å‘"
            - "xemxem'ç‰¹æ–¯æ‹‰'ç›¸å…³tin tá»©clÃ æ­£é¢cÃ²nlÃ è´Ÿé¢cá»§a"
            - "phÃºtæå„å¹³å°Ä‘á»‘i'trÃ­ tuá»‡ nhÃ¢n táº¡o'cá»§aæƒ…æ„Ÿæ€åº¦"
            - "xemxem'ç‰¹æ–¯æ‹‰'ç›¸å…³tin tá»©clÃ æ­£é¢cÃ²nlÃ è´Ÿé¢cá»§aï¼Œè¯·é€‰æ‹©má»™tå‘¨trongcá»§aå‰10tintin tá»©cÄ‘áº¿nphÃºtæ"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> tools = AnalyticsTools()
            >>> # phÃºtæhÃ´m naycá»§aç‰¹æ–¯æ‹‰tin tá»©cï¼Œè¿”å›å‰10tin
            >>> result = tools.analyze_sentiment(
            ...     topic="ç‰¹æ–¯æ‹‰",
            ...     limit=10
            ... )
            >>> # phÃºtæmá»™tå‘¨trongcá»§aç‰¹æ–¯æ‹‰tin tá»©cï¼ˆå‡è®¾hÃ´m naylÃ  2025-11-17ï¼‰
            >>> result = tools.analyze_sentiment(
            ...     topic="ç‰¹æ–¯æ‹‰",
            ...     date_range={"start": "2025-11-11", "end": "2025-11-17"},
            ...     limit=10
            ... )
            >>> print(result['ai_prompt'])  # Láº¥ytáº¡ocá»§aæç¤ºè¯
        """
        try:
            # tham sá»‘éªŒè¯
            if topic:
                topic = validate_keyword(topic)
            platforms = validate_platforms(platforms)
            limit = validate_limit(limit, default=50)

            # Xá»­ lÃ½ngÃ yèŒƒå›´
            if date_range:
                date_range_tuple = validate_date_range(date_range)
                start_date, end_date = date_range_tuple
            else:
                # é»˜è®¤hÃ´m nay
                start_date = end_date = datetime.now()

            # æ”¶é›†má»›ié—»dá»¯ liá»‡uï¼ˆæ”¯æŒå¤šå¤©ï¼‰
            all_news_items = []
            current_date = start_date

            while current_date <= end_date:
                try:
                    all_titles, id_to_name, _ = self.data_service.parser.read_all_titles_for_date(
                        date=current_date,
                        platform_ids=platforms
                    )

                    # æ”¶é›†è¯¥ngÃ ycá»§amá»›ié—»
                    for platform_id, titles in all_titles.items():
                        platform_name = id_to_name.get(platform_id, platform_id)
                        for title, info in titles.items():
                            # náº¿uæŒ‡å®šrá»“iè¯é¢˜ï¼Œåªæ”¶é›†åŒ…å«è¯é¢˜cá»§aæ ‡é¢˜
                            if topic and topic.lower() not in title.lower():
                                continue

                            news_item = {
                                "platform": platform_name,
                                "title": title,
                                "ranks": info.get("ranks", []),
                                "count": len(info.get("ranks", [])),
                                "date": current_date.strftime("%Y-%m-%d")
                            }

                            # tinä»¶æ€§thÃªm URL å­—æ®µ
                            if include_url:
                                news_item["url"] = info.get("url", "")
                                news_item["mobileUrl"] = info.get("mobileUrl", "")

                            all_news_items.append(news_item)

                except DataNotFoundError:
                    # è¯¥ngÃ ykhÃ´ng cÃ³dá»¯ liá»‡uï¼Œtiáº¿p tá»¥cdÆ°á»›imá»™tå¤©
                    pass

                # dÆ°á»›imá»™tå¤©
                current_date += timedelta(days=1)

            if not all_news_items:
                time_desc = "hÃ´m nay" if start_date == end_date else f"{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}"
                raise DataNotFoundError(
                    f"æœªæ‰¾Ä‘áº¿nç›¸å…³tin tá»©cï¼ˆ{time_desc}ï¼‰",
                    suggestion="è¯·å°è¯•å…¶ä»–è¯é¢˜ã€ngÃ yæœŸèŒƒå›´hoáº·cå¹³å°"
                )

            # Ä‘ié‡ï¼ˆåŒmá»™tæ ‡é¢˜åªgiá»¯ láº¡imá»™tláº§nï¼‰
            unique_news = {}
            for item in all_news_items:
                key = f"{item['platform']}::{item['title']}"
                if key not in unique_news:
                    unique_news[key] = item
                else:
                    # åˆå¹¶ ranksï¼ˆnáº¿uåŒmá»™tmá»›ié—»á»Ÿå¤šå¤©å‡ºç°ï¼‰
                    existing = unique_news[key]
                    existing["ranks"].extend(item["ranks"])
                    existing["count"] = len(existing["ranks"])

            deduplicated_news = list(unique_news.values())

            # Sáº¯p xáº¿p theo trá»ng sá»‘ï¼ˆnáº¿uå¯ç”¨ï¼‰
            if sort_by_weight:
                deduplicated_news.sort(
                    key=lambda x: calculate_news_weight(x),
                    reverse=True
                )

            # giá»›i háº¡nè¿”å›sá»‘ lÆ°á»£ng
            selected_news = deduplicated_news[:limit]

            # táº¡o AI æç¤ºè¯
            ai_prompt = self._create_sentiment_analysis_prompt(
                news_data=selected_news,
                topic=topic
            )

            # æ„å»ºthá»i gianèŒƒå›´æè¿°
            if start_date == end_date:
                time_range_desc = start_date.strftime("%Y-%m-%d")
            else:
                time_range_desc = f"{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}"

            result = {
                "success": True,
                "method": "ai_prompt_generation",
                "summary": {
                    "total_found": len(deduplicated_news),
                    "returned_count": len(selected_news),
                    "requested_limit": limit,
                    "duplicates_removed": len(all_news_items) - len(deduplicated_news),
                    "topic": topic,
                    "time_range": time_range_desc,
                    "platforms": list(set(item["platform"] for item in selected_news)),
                    "sorted_by_weight": sort_by_weight
                },
                "ai_prompt": ai_prompt,
                "news_sample": selected_news,
                "usage_note": "è¯·sáº½ ai_prompt å­—æ®µcá»§aná»™i dunggá»­iç»™ AI è¿›è¡Œæƒ…æ„ŸphÃºtæ"
            }

            # náº¿uè¿”å›sá»‘ lÆ°á»£ngå°‘á»ŸYÃªu cáº§usá»‘ lÆ°á»£ngï¼Œå¢åŠ æç¤º
            if len(selected_news) < limit and len(deduplicated_news) >= limit:
                result["note"] = "è¿”å›æ•°é‡å°‘á»Ÿè¯·æ±‚æ•°é‡lÃ vÃ¬Ä‘ié‡é€»è¾‘ï¼ˆåŒmá»™tæ ‡é¢˜á»ŸkhÃ´ngåŒå¹³å°åªgiá»¯ láº¡imá»™tláº§nï¼‰"
            elif len(deduplicated_news) < limit:
                result["note"] = f"á»ŸæŒ‡å®šgiá»é—´èŒƒå›´trongä»…æ‰¾Ä‘áº¿n {len(deduplicated_news)} tinkhá»›pcá»§atin tá»©c"

            return result

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def _create_sentiment_analysis_prompt(
        self,
        news_data: List[Dict],
        topic: Optional[str]
    ) -> str:
        """
        táº¡oæƒ…æ„ŸphÃºtæcá»§a AI æç¤ºè¯

        Args:
            news_data: tin tá»©cæ•°æ®åˆ—è¡¨ï¼ˆÄ‘Ã£æ’åºvÃ giá»›i háº¡næ•°é‡ï¼‰
            topic: è¯é¢˜å…³é”®è¯

        Returns:
            Ä‘á»‹nh dáº¡ngåŒ–cá»§a AI æç¤ºè¯
        """
        # theoå¹³å°phÃºtç»„
        platform_news = defaultdict(list)
        for item in news_data:
            platform_news[item["platform"]].append({
                "title": item["title"],
                "date": item.get("date", "")
            })

        # æ„å»ºæç¤ºè¯
        prompt_parts = []

        # 1. ä»»åŠ¡nÃ³iæ˜
        if topic:
            prompt_parts.append(f"è¯·phÃºtæÄ‘á»ƒdÆ°á»›iå…³á»Ÿã€Œ{topic}ã€cá»§atin tá»©cæ ‡é¢˜cá»§aæƒ…æ„Ÿå€¾å‘ã€‚")
        else:
            prompt_parts.append("è¯·phÃºtæÄ‘á»ƒdÆ°á»›itin tá»©cæ ‡é¢˜cá»§aæƒ…æ„Ÿå€¾å‘ã€‚")

        prompt_parts.append("")
        prompt_parts.append("phÃºtæmuá»‘næ±‚ï¼š")
        prompt_parts.append("1. è¯†åˆ«æ¯tintin tá»©ccá»§aæƒ…æ„Ÿå€¾å‘ï¼ˆæ­£é¢/è´Ÿé¢/trongæ€§ï¼‰")
        prompt_parts.append("2. ç»Ÿè®¡å„æƒ…æ„Ÿç±»åˆ«cá»§aæ•°é‡vÃ ç™¾phÃºtæ¯”")
        prompt_parts.append("3. phÃºtækhÃ´ngåŒå¹³å°cá»§aæƒ…æ„Ÿå·®å¼‚")
        prompt_parts.append("4. æ€»ç»“æ•´ä½“æƒ…æ„Ÿè¶‹åŠ¿")
        prompt_parts.append("5. åˆ—ä¸¾å…¸å‹cá»§aæ­£é¢vÃ è´Ÿé¢tin tá»©cæ ·æœ¬")
        prompt_parts.append("")

        # 2. dá»¯ liá»‡uæ¦‚è§ˆ
        prompt_parts.append(f"æ•°æ®æ¦‚è§ˆï¼š")
        prompt_parts.append(f"- Tá»•ng sá»‘ tin tá»©cï¼š{len(news_data)}")
        prompt_parts.append(f"- è¦†ç›–å¹³å°ï¼š{len(platform_news)}")

        # thá»i gianèŒƒå›´
        dates = set(item.get("date", "") for item in news_data if item.get("date"))
        if dates:
            date_list = sorted(dates)
            if len(date_list) == 1:
                prompt_parts.append(f"- giá»é—´èŒƒå›´ï¼š{date_list[0]}")
            else:
                prompt_parts.append(f"- giá»é—´èŒƒå›´ï¼š{date_list[0]} è‡³ {date_list[-1]}")

        prompt_parts.append("")

        # 3. theoå¹³å°å±•ç¤ºmá»›ié—»
        prompt_parts.append("tin tá»©cåˆ—è¡¨ï¼ˆtheoå¹³å°phÃºtç±»ï¼ŒÄ‘Ã£theoé‡muá»‘næ€§æ’åºï¼‰ï¼š")
        prompt_parts.append("")

        for platform, items in sorted(platform_news.items()):
            prompt_parts.append(f"ã€{platform}ã€‘({len(items)} tin)")
            for i, item in enumerate(items, 1):
                title = item["title"]
                date_str = f" [{item['date']}]" if item.get("date") else ""
                prompt_parts.append(f"{i}. {title}{date_str}")
            prompt_parts.append("")

        # 4. è¾“å‡ºÄ‘á»‹nh dáº¡ngnÃ³iæ˜
        prompt_parts.append("è¯·theoÄ‘á»ƒdÆ°á»›iÄ‘á»‹nh dáº¡ngè¾“å‡ºphÃºtæç»“æœï¼š")
        prompt_parts.append("")
        prompt_parts.append("## æƒ…æ„ŸphÃºtå¸ƒç»Ÿè®¡")
        prompt_parts.append("- æ­£é¢ï¼šXXtin (XX%)")
        prompt_parts.append("- è´Ÿé¢ï¼šXXtin (XX%)")
        prompt_parts.append("- trongæ€§ï¼šXXtin (XX%)")
        prompt_parts.append("")
        prompt_parts.append("## å¹³å°æƒ…æ„ŸÄ‘á»‘iæ¯”")
        prompt_parts.append("[å„å¹³å°cá»§aæƒ…æ„Ÿå€¾å‘å·®å¼‚]")
        prompt_parts.append("")
        prompt_parts.append("## æ•´ä½“æƒ…æ„Ÿè¶‹åŠ¿")
        prompt_parts.append("[æ€»ä½“phÃºtævÃ å…³é”®å‘ç°]")
        prompt_parts.append("")
        prompt_parts.append("## å…¸å‹æ ·æœ¬")
        prompt_parts.append("æ­£é¢tin tá»©cæ ·æœ¬ï¼š")
        prompt_parts.append("[åˆ—ä¸¾3-5tin]")
        prompt_parts.append("")
        prompt_parts.append("è´Ÿé¢tin tá»©cæ ·æœ¬ï¼š")
        prompt_parts.append("[åˆ—ä¸¾3-5tin]")

        return "\n".join(prompt_parts)

    def find_similar_news(
        self,
        reference_title: str,
        threshold: float = 0.6,
        limit: int = 50,
        include_url: bool = False
    ) -> Dict:
        """
        ç›¸ä¼¼tin tá»©cæŸ¥æ‰¾ - åŸºá»Ÿæ ‡é¢˜ç›¸ä¼¼åº¦æŸ¥æ‰¾ç›¸å…³tin tá»©c

        Args:
            reference_title: å‚è€ƒæ ‡é¢˜
            threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ä¹‹é—´ï¼‰
            limit: è¿”å›tinæ•°giá»›i háº¡nï¼Œé»˜è®¤50
            include_url: lÃ å¦åŒ…å«URLé“¾æ¥ï¼Œé»˜è®¤Falseï¼ˆèŠ‚çœtokenï¼‰

        Returns:
            ç›¸ä¼¼tin tá»©cåˆ—è¡¨

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "æ‰¾å‡ºvÃ 'ç‰¹æ–¯æ‹‰é™ä»·'ç›¸ä¼¼cá»§atin tá»©c"
            - "æŸ¥æ‰¾å…³á»ŸiPhoneå‘å¸ƒcá»§aç±»ä¼¼æŠ¥é“"
            - "xemxemcÃ³khÃ´ng cÃ³vÃ nÃ ytintin tá»©cç›¸ä¼¼cá»§aæŠ¥é“"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> tools = AnalyticsTools()
            >>> result = tools.find_similar_news(
            ...     reference_title="ç‰¹æ–¯æ‹‰å®£å¸ƒé™ä»·",
            ...     threshold=0.6,
            ...     limit=10
            ... )
            >>> print(result['similar_news'])
        """
        try:
            # tham sá»‘éªŒè¯
            reference_title = validate_keyword(reference_title)

            if not 0 <= threshold <= 1:
                raise InvalidParameterError(
                    "threshold å¿…é¡»á»Ÿ 0 Ä‘áº¿n 1 ä¹‹é—´",
                    suggestion="æ¨èå€¼ï¼š0.5-0.8"
                )

            limit = validate_limit(limit, default=50)

            # Ä‘á»cdá»¯ liá»‡u
            all_titles, id_to_name, _ = self.data_service.parser.read_all_titles_for_date()

            # tÃ­nh toÃ¡nç›¸ä¼¼åº¦
            similar_items = []

            for platform_id, titles in all_titles.items():
                platform_name = id_to_name.get(platform_id, platform_id)

                for title, info in titles.items():
                    if title == reference_title:
                        continue

                    # tÃ­nh toÃ¡nç›¸ä¼¼åº¦
                    similarity = self._calculate_similarity(reference_title, title)

                    if similarity >= threshold:
                        news_item = {
                            "title": title,
                            "platform": platform_id,
                            "platform_name": platform_name,
                            "similarity": round(similarity, 3),
                            "rank": info["ranks"][0] if info["ranks"] else 0
                        }

                        # tinä»¶æ€§thÃªm URL å­—æ®µ
                        if include_url:
                            news_item["url"] = info.get("url", "")

                        similar_items.append(news_item)

            # theoç›¸ä¼¼åº¦sáº¯p xáº¿p
            similar_items.sort(key=lambda x: x["similarity"], reverse=True)

            # giá»›i háº¡nsá»‘ lÆ°á»£ng
            result_items = similar_items[:limit]

            if not result_items:
                raise DataNotFoundError(
                    f"æœªæ‰¾Ä‘áº¿nç›¸ä¼¼åº¦è¶…è¿‡ {threshold} cá»§atin tá»©c",
                    suggestion="è¯·é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼hoáº·cå°è¯•å…¶ä»–æ ‡é¢˜"
                )

            result = {
                "success": True,
                "summary": {
                    "total_found": len(similar_items),
                    "returned_count": len(result_items),
                    "requested_limit": limit,
                    "threshold": threshold,
                    "reference_title": reference_title
                },
                "similar_news": result_items
            }

            if len(similar_items) < limit:
                result["note"] = f"ç›¸ä¼¼åº¦é˜ˆå€¼ {threshold} dÆ°á»›iä»…æ‰¾Ä‘áº¿n {len(similar_items)} tinç›¸ä¼¼tin tá»©c"

            return result

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def search_by_entity(
        self,
        entity: str,
        entity_type: Optional[str] = None,
        limit: int = 50,
        sort_by_weight: bool = True
    ) -> Dict:
        """
        å®ä½“è¯†åˆ«æœç´¢ - æœç´¢åŒ…å«ç‰¹å®šngÆ°á»iç‰©/åœ°ç‚¹/æœºæ„cá»§atin tá»©c

        Args:
            entity: å®ä½“åç§°
            entity_type: å®ä½“loáº¡iï¼ˆperson/location/organizationï¼‰ï¼ŒcÃ³ thá»ƒé€‰
            limit: è¿”å›tinæ•°giá»›i háº¡nï¼Œé»˜è®¤50ï¼Œnháº¥tå¤§200
            sort_by_weight: lÃ å¦theoæƒé‡æ’åºï¼Œé»˜è®¤True

        Returns:
            å®ä½“ç›¸å…³tin tá»©cåˆ—è¡¨

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "æœç´¢é©¬æ–¯å…‹ç›¸å…³cá»§atin tá»©c"
            - "æŸ¥æ‰¾å…³á»Ÿç‰¹æ–¯æ‹‰å…¬å¸cá»§aæŠ¥é“ï¼Œè¿”å›å‰20tin"
            - "xemxemåŒ—äº¬cÃ³ä»€ä¹ˆtin tá»©c"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> tools = AnalyticsTools()
            >>> result = tools.search_by_entity(
            ...     entity="é©¬æ–¯å…‹",
            ...     entity_type="person",
            ...     limit=20
            ... )
            >>> print(result['related_news'])
        """
        try:
            # tham sá»‘éªŒè¯
            entity = validate_keyword(entity)
            limit = validate_limit(limit, default=50)

            if entity_type and entity_type not in ["person", "location", "organization"]:
                raise InvalidParameterError(
                    f"æ— æ•ˆcá»§aå®ä½“loáº¡i: {entity_type}",
                    suggestion="æ”¯æŒcá»§aloáº¡i: person, location, organization"
                )

            # Ä‘á»cdá»¯ liá»‡u
            all_titles, id_to_name, _ = self.data_service.parser.read_all_titles_for_date()

            # tÃ¬m kiáº¿måŒ…å«å®ä½“cá»§amá»›ié—»
            related_news = []
            entity_context = Counter()  # ç»Ÿè®¡å®ä½“å‘¨è¾¹cá»§aè¯

            for platform_id, titles in all_titles.items():
                platform_name = id_to_name.get(platform_id, platform_id)

                for title, info in titles.items():
                    if entity in title:
                        url = info.get("url", "")
                        mobile_url = info.get("mobileUrl", "")
                        ranks = info.get("ranks", [])
                        count = len(ranks)

                        related_news.append({
                            "title": title,
                            "platform": platform_id,
                            "platform_name": platform_name,
                            "url": url,
                            "mobileUrl": mobile_url,
                            "ranks": ranks,
                            "count": count,
                            "rank": ranks[0] if ranks else 999
                        })

                        # TrÃ­ch xuáº¥tå®ä½“å‘¨è¾¹cá»§aå…³é”®è¯
                        keywords = self._extract_keywords(title)
                        entity_context.update(keywords)

            if not related_news:
                raise DataNotFoundError(
                    f"æœªæ‰¾Ä‘áº¿nåŒ…å«å®ä½“ '{entity}' cá»§atin tá»©c",
                    suggestion="è¯·å°è¯•å…¶ä»–å®ä½“åç§°"
                )

            # ç§»é™¤å®ä½“æœ¬èº«
            if entity in entity_context:
                del entity_context[entity]

            # Sáº¯p xáº¿p theo trá»ng sá»‘ï¼ˆnáº¿uå¯ç”¨ï¼‰
            if sort_by_weight:
                related_news.sort(
                    key=lambda x: calculate_news_weight(x),
                    reverse=True
                )
            else:
                # theoæ’åsáº¯p xáº¿p
                related_news.sort(key=lambda x: x["rank"])

            # giá»›i háº¡nè¿”å›sá»‘ lÆ°á»£ng
            result_news = related_news[:limit]

            return {
                "success": True,
                "entity": entity,
                "entity_type": entity_type or "auto",
                "related_news": result_news,
                "total_found": len(related_news),
                "returned_count": len(result_news),
                "sorted_by_weight": sort_by_weight,
                "related_keywords": [
                    {"keyword": k, "count": v}
                    for k, v in entity_context.most_common(10)
                ]
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def generate_summary_report(
        self,
        report_type: str = "daily",
        date_range: Optional[Dict[str, str]] = None
    ) -> Dict:
        """
        æ¯ngÃ y/æ¯å‘¨æ‘˜muá»‘ntáº¡oå™¨ - è‡ªåŠ¨táº¡oxu hÆ°á»›ng nÃ³ngæ‘˜muá»‘nbÃ¡o cÃ¡o

        Args:
            report_type: Loáº¡i bÃ¡o cÃ¡oï¼ˆdaily/weeklyï¼‰
            date_range: tÃ¹y chá»‰nhngÃ yæœŸèŒƒå›´ï¼ˆcÃ³ thá»ƒé€‰ï¼‰

        Returns:
            MarkdownÄ‘á»‹nh dáº¡ngcá»§aæ‘˜muá»‘nbÃ¡o cÃ¡o

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "táº¡ohÃ´m naycá»§atin tá»©cæ‘˜muá»‘nbÃ¡o cÃ¡o"
            - "ç»™tÃ´imá»™tä»½æœ¬å‘¨cá»§axu hÆ°á»›ng nÃ³ngæ€»ç»“"
            - "táº¡oè¿‡Ä‘i7å¤©cá»§atin tá»©cphÃºtæbÃ¡o cÃ¡o"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> tools = AnalyticsTools()
            >>> result = tools.generate_summary_report(
            ...     report_type="daily"
            ... )
            >>> print(result['markdown_report'])
        """
        try:
            # tham sá»‘éªŒè¯
            if report_type not in ["daily", "weekly"]:
                raise InvalidParameterError(
                    f"æ— æ•ˆcá»§aLoáº¡i bÃ¡o cÃ¡o: {report_type}",
                    suggestion="æ”¯æŒcá»§aloáº¡i: daily, weekly"
                )

            # ç¡®å®šngÃ yèŒƒå›´
            if date_range:
                date_range_tuple = validate_date_range(date_range)
                start_date, end_date = date_range_tuple
            else:
                if report_type == "daily":
                    start_date = end_date = datetime.now()
                else:  # weekly
                    end_date = datetime.now()
                    start_date = end_date - timedelta(days=6)

            # æ”¶é›†dá»¯ liá»‡u
            all_keywords = Counter()
            all_platforms_news = defaultdict(int)
            all_titles_list = []

            current_date = start_date
            while current_date <= end_date:
                try:
                    all_titles, id_to_name, _ = self.data_service.parser.read_all_titles_for_date(
                        date=current_date
                    )

                    for platform_id, titles in all_titles.items():
                        platform_name = id_to_name.get(platform_id, platform_id)
                        all_platforms_news[platform_name] += len(titles)

                        for title in titles.keys():
                            all_titles_list.append({
                                "title": title,
                                "platform": platform_name,
                                "date": current_date.strftime("%Y-%m-%d")
                            })

                            # TrÃ­ch xuáº¥tå…³é”®è¯
                            keywords = self._extract_keywords(title)
                            all_keywords.update(keywords)

                except DataNotFoundError:
                    pass

                current_date += timedelta(days=1)

            # táº¡obÃ¡o cÃ¡o
            report_title = f"{'æ¯ngÃ y' if report_type == 'daily' else 'æ¯å‘¨'}tin tá»©cxu hÆ°á»›ng nÃ³ngæ‘˜muá»‘n"
            date_str = f"{start_date.strftime('%Y-%m-%d')}" if report_type == "daily" else f"{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}"

            # æ„å»ºMarkdownbÃ¡o cÃ¡o
            markdown = f"""# {report_title}

**bÃ¡o cÃ¡ongÃ yæœŸ**: {date_str}
**táº¡ogiá»é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸ“Š dá»¯ liá»‡uæ¦‚è§ˆ

- **Tá»•ng sá»‘ tin tá»©c**: {len(all_titles_list)}
- **è¦†ç›–å¹³å°**: {len(all_platforms_news)}
- **çƒ­é—¨å…³é”®è¯æ•°**: {len(all_keywords)}

## ğŸ”¥ TOP 10 çƒ­é—¨è¯é¢˜

"""

            # thÃªmTOP 10å…³é”®è¯
            for i, (keyword, count) in enumerate(all_keywords.most_common(10), 1):
                markdown += f"{i}. **{keyword}** - å‡ºç° {count} láº§n\n"

            # å¹³å°phÃ¢n tÃ­ch
            markdown += "\n## ğŸ“± å¹³å°æ´»è·ƒåº¦\n\n"
            sorted_platforms = sorted(all_platforms_news.items(), key=lambda x: x[1], reverse=True)

            for platform, count in sorted_platforms:
                markdown += f"- **{platform}**: {count} tintin tá»©c\n"

            # è¶‹åŠ¿å˜åŒ–ï¼ˆNáº¿u lÃ å‘¨æŠ¥ï¼‰
            if report_type == "weekly":
                markdown += "\n## ğŸ“ˆ è¶‹åŠ¿phÃºtæ\n\n"
                markdown += "æœ¬å‘¨çƒ­åº¦æŒç»­cá»§aè¯é¢˜ï¼ˆæ ·æœ¬æ•°æ®ï¼‰ï¼š\n\n"

                # ç®€å•cá»§aè¶‹åŠ¿phÃ¢n tÃ­ch
                top_keywords = [kw for kw, _ in all_keywords.most_common(5)]
                for keyword in top_keywords:
                    markdown += f"- **{keyword}**: æŒç»­çƒ­é—¨\n"

            # thÃªmæ ·æœ¬má»›ié—»ï¼ˆtheoæƒé‡é€‰æ‹©ï¼ŒÄ‘áº£m báº£oç¡®å®šæ€§ï¼‰
            markdown += "\n## ğŸ“° ç²¾é€‰tin tá»©cæ ·æœ¬\n\n"

            # ç¡®å®šæ€§é€‰å–ï¼štheoæ ‡é¢˜cá»§aæƒé‡sáº¯p xáº¿pï¼Œå–å‰5tin
            # nÃ yæ ·ç›¸åŒè¾“å…¥æ€»lÃ è¿”å›ç›¸åŒç»“æœ
            if all_titles_list:
                # tÃ­nh toÃ¡næ¯tin tá»©ccá»§aæƒé‡phÃºtæ•°ï¼ˆåŸºá»Ÿå…³é”®è¯Sá»‘ láº§n xuáº¥t hiá»‡nï¼‰
                news_with_scores = []
                for news in all_titles_list:
                    # ç®€å•æƒé‡ï¼šthá»‘ng kÃªåŒ…å«TOPå…³é”®è¯cá»§aláº§næ•°
                    score = 0
                    title_lower = news['title'].lower()
                    for keyword, count in all_keywords.most_common(10):
                        if keyword.lower() in title_lower:
                            score += count
                    news_with_scores.append((news, score))

                # theoæƒé‡é™åºsáº¯p xáº¿pï¼Œæƒé‡ç›¸åŒåˆ™theoæ ‡é¢˜å­—æ¯é¡ºåºï¼ˆÄ‘áº£m báº£oç¡®å®šæ€§ï¼‰
                news_with_scores.sort(key=lambda x: (-x[1], x[0]['title']))

                # å–å‰5tin
                sample_news = [item[0] for item in news_with_scores[:5]]

                for news in sample_news:
                    markdown += f"- [{news['platform']}] {news['title']}\n"

            markdown += "\n---\n\n*æœ¬bÃ¡o cÃ¡odo TrendRadar MCP è‡ªåŠ¨táº¡o*\n"

            return {
                "success": True,
                "report_type": report_type,
                "date_range": {
                    "start": start_date.strftime("%Y-%m-%d"),
                    "end": end_date.strftime("%Y-%m-%d")
                },
                "markdown_report": markdown,
                "statistics": {
                    "total_news": len(all_titles_list),
                    "platforms_count": len(all_platforms_news),
                    "keywords_count": len(all_keywords),
                    "top_keyword": all_keywords.most_common(1)[0] if all_keywords else None
                }
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def get_platform_activity_stats(
        self,
        date_range: Optional[Dict[str, str]] = None
    ) -> Dict:
        """
        å¹³å°æ´»è·ƒåº¦ç»Ÿè®¡ - ç»Ÿè®¡å„å¹³å°cá»§aå‘å¸ƒé¢‘ç‡vÃ æ´»è·ƒgiá»é—´æ®µ

        Args:
            date_range: ngÃ yæœŸèŒƒå›´ï¼ˆcÃ³ thá»ƒé€‰ï¼‰

        Returns:
            å¹³å°æ´»è·ƒåº¦ç»Ÿè®¡ç»“æœ

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "ç»Ÿè®¡å„å¹³å°hÃ´m naycá»§aæ´»è·ƒåº¦"
            - "xemxemå“ªä¸ªå¹³å°hÆ¡næ–°nháº¥té¢‘ç¹"
            - "phÃºtæå„å¹³å°cá»§aå‘å¸ƒgiá»é—´è§„å¾‹"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> # æŸ¥xemå„å¹³å°æ´»è·ƒåº¦ï¼ˆå‡è®¾hÃ´m naylÃ  2025-11-17ï¼‰
            >>> result = tools.get_platform_activity_stats(
            ...     date_range={"start": "2025-11-08", "end": "2025-11-17"}
            ... )
            >>> print(result['platform_activity'])
        """
        try:
            # tham sá»‘éªŒè¯
            date_range_tuple = validate_date_range(date_range)

            # ç¡®å®šngÃ yèŒƒå›´
            if date_range_tuple:
                start_date, end_date = date_range_tuple
            else:
                start_date = end_date = datetime.now()

            # thá»‘ng kÃªå„å¹³å°æ´»è·ƒåº¦
            platform_activity = defaultdict(lambda: {
                "total_updates": 0,
                "days_active": set(),
                "news_count": 0,
                "hourly_distribution": Counter()
            })

            # éå†ngÃ yèŒƒå›´
            current_date = start_date
            while current_date <= end_date:
                try:
                    all_titles, id_to_name, timestamps = self.data_service.parser.read_all_titles_for_date(
                        date=current_date
                    )

                    for platform_id, titles in all_titles.items():
                        platform_name = id_to_name.get(platform_id, platform_id)

                        platform_activity[platform_name]["news_count"] += len(titles)
                        platform_activity[platform_name]["days_active"].add(current_date.strftime("%Y-%m-%d"))

                        # thá»‘ng kÃªcáº­p nháº­tláº§næ•°ï¼ˆåŸºá»Ÿfilesá»‘ lÆ°á»£ngï¼‰
                        platform_activity[platform_name]["total_updates"] += len(timestamps)

                        # thá»‘ng kÃªthá»i gianphÃºtå¸ƒï¼ˆåŸºá»Ÿfileåtrongcá»§athá»i gianï¼‰
                        for filename in timestamps.keys():
                            # PhÃ¢n tÃ­chfileåtrongcá»§aå°giá»ï¼ˆÄ‘á»‹nh dáº¡ngï¼šHHMM.txtï¼‰
                            match = re.match(r'(\d{2})(\d{2})\.txt', filename)
                            if match:
                                hour = int(match.group(1))
                                platform_activity[platform_name]["hourly_distribution"][hour] += 1

                except DataNotFoundError:
                    pass

                current_date += timedelta(days=1)

            # è½¬æ¢vÃ¬cÃ³ thá»ƒåºåˆ—åŒ–cá»§aÄ‘á»‹nh dáº¡ng
            result_activity = {}
            for platform, stats in platform_activity.items():
                days_count = len(stats["days_active"])
                avg_news_per_day = stats["news_count"] / days_count if days_count > 0 else 0

                # æ‰¾å‡ºnháº¥tæ´»è·ƒcá»§athá»i gianæ®µ
                most_active_hours = stats["hourly_distribution"].most_common(3)

                result_activity[platform] = {
                    "total_updates": stats["total_updates"],
                    "news_count": stats["news_count"],
                    "days_active": days_count,
                    "avg_news_per_day": round(avg_news_per_day, 2),
                    "most_active_hours": [
                        {"hour": f"{hour:02d}:00", "count": count}
                        for hour, count in most_active_hours
                    ],
                    "activity_score": round(stats["news_count"] / max(days_count, 1), 2)
                }

            # theoæ´»è·ƒåº¦sáº¯p xáº¿p
            sorted_platforms = sorted(
                result_activity.items(),
                key=lambda x: x[1]["activity_score"],
                reverse=True
            )

            return {
                "success": True,
                "date_range": {
                    "start": start_date.strftime("%Y-%m-%d"),
                    "end": end_date.strftime("%Y-%m-%d")
                },
                "platform_activity": dict(sorted_platforms),
                "most_active_platform": sorted_platforms[0][0] if sorted_platforms else None,
                "total_platforms": len(result_activity)
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def analyze_topic_lifecycle(
        self,
        topic: str,
        date_range: Optional[Dict[str, str]] = None
    ) -> Dict:
        """
        è¯é¢˜ç”Ÿå‘½å‘¨æœŸphÃºtæ - è¿½è¸ªè¯é¢˜tá»«å‡ºç°Ä‘áº¿næ¶ˆå¤±cá»§aå®Œæ•´å‘¨æœŸ

        Args:
            topic: è¯é¢˜å…³é”®è¯
            date_range: ngÃ yæœŸèŒƒå›´ï¼ˆcÃ³ thá»ƒé€‰ï¼‰
                       - **Ä‘á»‹nh dáº¡ng**: {"start": "YYYY-MM-DD", "end": "YYYY-MM-DD"}
                       - **é»˜è®¤**: khÃ´ngæŒ‡å®šgiá»é»˜è®¤phÃºtænháº¥tè¿‘7å¤©

        Returns:
            è¯é¢˜ç”Ÿå‘½å‘¨æœŸphÃºtæç»“æœ

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "phÃºtæ'trÃ­ tuá»‡ nhÃ¢n táº¡o'nÃ yä¸ªè¯é¢˜cá»§aç”Ÿå‘½å‘¨æœŸ"
            - "xemxem'iPhone'è¯é¢˜lÃ æ˜™èŠ±má»™tç°cÃ²nlÃ æŒç»­xu hÆ°á»›ng nÃ³ng"
            - "è¿½è¸ª'æ¯”ç‰¹å¸'è¯é¢˜cá»§açƒ­åº¦å˜åŒ–"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> # phÃºtæè¯é¢˜ç”Ÿå‘½å‘¨æœŸï¼ˆå‡è®¾hÃ´m naylÃ  2025-11-17ï¼‰
            >>> result = tools.analyze_topic_lifecycle(
            ...     topic="trÃ­ tuá»‡ nhÃ¢n táº¡o",
            ...     date_range={"start": "2025-10-19", "end": "2025-11-17"}
            ... )
            >>> print(result['lifecycle_stage'])
        """
        try:
            # tham sá»‘éªŒè¯
            topic = validate_keyword(topic)

            # Xá»­ lÃ½ngÃ yèŒƒå›´ï¼ˆkhÃ´ngæŒ‡å®šgiá»é»˜è®¤nháº¥tè¿‘7å¤©ï¼‰
            if date_range:
                from ..utils.validators import validate_date_range
                date_range_tuple = validate_date_range(date_range)
                start_date, end_date = date_range_tuple
            else:
                # é»˜è®¤nháº¥tè¿‘7å¤©
                end_date = datetime.now()
                start_date = end_date - timedelta(days=6)

            # æ”¶é›†è¯é¢˜lá»‹ch sá»­dá»¯ liá»‡u
            lifecycle_data = []
            current_date = start_date
            while current_date <= end_date:
                try:
                    all_titles, _, _ = self.data_service.parser.read_all_titles_for_date(
                        date=current_date
                    )

                    # thá»‘ng kÃªè¯¥ngÃ ycá»§aè¯é¢˜Sá»‘ láº§n xuáº¥t hiá»‡n
                    count = 0
                    for _, titles in all_titles.items():
                        for title in titles.keys():
                            if topic.lower() in title.lower():
                                count += 1

                    lifecycle_data.append({
                        "date": current_date.strftime("%Y-%m-%d"),
                        "count": count
                    })

                except DataNotFoundError:
                    lifecycle_data.append({
                        "date": current_date.strftime("%Y-%m-%d"),
                        "count": 0
                    })

                current_date += timedelta(days=1)

            # tÃ­nh toÃ¡nphÃ¢n tÃ­chå¤©æ•°
            total_days = (end_date - start_date).days + 1

            # phÃ¢n tÃ­chç”Ÿå‘½å‘¨æœŸé˜¶æ®µ
            counts = [item["count"] for item in lifecycle_data]

            if not any(counts):
                time_desc = f"{start_date.strftime('%Y-%m-%d')} è‡³ {end_date.strftime('%Y-%m-%d')}"
                raise DataNotFoundError(
                    f"á»Ÿ {time_desc} trongæœªæ‰¾Ä‘áº¿nè¯é¢˜ '{topic}'",
                    suggestion="è¯·å°è¯•å…¶ä»–è¯é¢˜hoáº·cæ‰©å¤§giá»é—´èŒƒå›´"
                )

            # æ‰¾Ä‘áº¿né¦–láº§nå‡ºç°vÃ cuá»‘i cÃ¹ngå‡ºç°
            first_appearance = next((item["date"] for item in lifecycle_data if item["count"] > 0), None)
            last_appearance = next((item["date"] for item in reversed(lifecycle_data) if item["count"] > 0), None)

            # tÃ­nh toÃ¡nå³°giÃ¡ trá»‹
            max_count = max(counts)
            peak_index = counts.index(max_count)
            peak_date = lifecycle_data[peak_index]["date"]

            # tÃ­nh toÃ¡nå¹³å‡giÃ¡ trá»‹vÃ æ ‡å‡†å·®ï¼ˆç®€å•å®ç°ï¼‰
            non_zero_counts = [c for c in counts if c > 0]
            avg_count = sum(non_zero_counts) / len(non_zero_counts) if non_zero_counts else 0

            # åˆ¤æ–­ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ
            recent_counts = counts[-3:]  # nháº¥tè¿‘3å¤©
            early_counts = counts[:3]    # å‰3å¤©

            if sum(recent_counts) > sum(early_counts):
                lifecycle_stage = "trÃªnå‡æœŸ"
            elif sum(recent_counts) < sum(early_counts) * 0.5:
                lifecycle_stage = "è¡°é€€æœŸ"
            elif max_count in recent_counts:
                lifecycle_stage = "çˆ†å‘æœŸ"
            else:
                lifecycle_stage = "ç¨³å®šæœŸ"

            # phÃºtlá»›pï¼šæ˜™èŠ±má»™tç° vs æŒç»­xu hÆ°á»›ng nÃ³ng
            active_days = sum(1 for c in counts if c > 0)

            if active_days <= 2 and max_count > avg_count * 2:
                topic_type = "æ˜™èŠ±má»™tç°"
            elif active_days >= total_days * 0.6:
                topic_type = "æŒç»­xu hÆ°á»›ng nÃ³ng"
            else:
                topic_type = "å‘¨æœŸæ€§xu hÆ°á»›ng nÃ³ng"

            return {
                "success": True,
                "topic": topic,
                "date_range": {
                    "start": start_date.strftime("%Y-%m-%d"),
                    "end": end_date.strftime("%Y-%m-%d"),
                    "total_days": total_days
                },
                "lifecycle_data": lifecycle_data,
                "analysis": {
                    "first_appearance": first_appearance,
                    "last_appearance": last_appearance,
                    "peak_date": peak_date,
                    "peak_count": max_count,
                    "active_days": active_days,
                    "avg_daily_mentions": round(avg_count, 2),
                    "lifecycle_stage": lifecycle_stage,
                    "topic_type": topic_type
                }
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def detect_viral_topics(
        self,
        threshold: float = 3.0,
        time_window: int = 24
    ) -> Dict:
        """
        ngoáº¡i lá»‡çƒ­åº¦æ£€æµ‹ - è‡ªåŠ¨è¯†åˆ«çªç„¶çˆ†ç«cá»§aè¯é¢˜

        Args:
            threshold: çƒ­åº¦çªå¢å€æ•°é˜ˆå€¼
            time_window: æ£€æµ‹giá»é—´çª—å£ï¼ˆå°giá»ï¼‰

        Returns:
            çˆ†ç«è¯é¢˜åˆ—è¡¨

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "æ£€æµ‹hÃ´m naycÃ³å“ªäº›çªç„¶çˆ†ç«cá»§aè¯é¢˜"
            - "xemxemcÃ³khÃ´ng cÃ³çƒ­åº¦ngoáº¡i lá»‡cá»§atin tá»©c"
            - "é¢„è­¦cÃ³ thá»ƒèƒ½cá»§aé‡å¤§äº‹ä»¶"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> tools = AnalyticsTools()
            >>> result = tools.detect_viral_topics(
            ...     threshold=3.0,
            ...     time_window=24
            ... )
            >>> print(result['viral_topics'])
        """
        try:
            # tham sá»‘éªŒè¯
            if threshold < 1.0:
                raise InvalidParameterError(
                    "threshold å¿…é¡»å¤§á»Ÿv.v.á»Ÿ 1.0",
                    suggestion="æ¨èå€¼ï¼š2.0-5.0"
                )

            time_window = validate_limit(time_window, default=24, max_limit=72)

            # Ä‘á»chiá»‡n táº¡ivÃ ä¹‹å‰cá»§adá»¯ liá»‡u
            current_all_titles, _, _ = self.data_service.parser.read_all_titles_for_date()

            # Ä‘á»chÃ´m quacá»§adá»¯ liá»‡uä½œvÃ¬åŸºå‡†
            yesterday = datetime.now() - timedelta(days=1)
            try:
                previous_all_titles, _, _ = self.data_service.parser.read_all_titles_for_date(
                    date=yesterday
                )
            except DataNotFoundError:
                previous_all_titles = {}

            # thá»‘ng kÃªhiá»‡n táº¡icá»§aå…³é”®è¯é¢‘ç‡
            current_keywords = Counter()
            current_keyword_titles = defaultdict(list)

            for _, titles in current_all_titles.items():
                for title in titles.keys():
                    keywords = self._extract_keywords(title)
                    current_keywords.update(keywords)

                    for kw in keywords:
                        current_keyword_titles[kw].append(title)

            # thá»‘ng kÃªä¹‹å‰cá»§aå…³é”®è¯é¢‘ç‡
            previous_keywords = Counter()

            for _, titles in previous_all_titles.items():
                for title in titles.keys():
                    keywords = self._extract_keywords(title)
                    previous_keywords.update(keywords)

            # æ£€æµ‹ngoáº¡i lá»‡çƒ­åº¦
            viral_topics = []

            for keyword, current_count in current_keywords.items():
                previous_count = previous_keywords.get(keyword, 0)

                # tÃ­nh toÃ¡nå¢é•¿å€æ•°
                if previous_count == 0:
                    # má»›iå‡ºç°cá»§aè¯é¢˜
                    if current_count >= 5:  # Ã­t nháº¥tå‡ºç°5láº§næ‰è®¤vÃ¬lÃ çˆ†ç«
                        growth_rate = float('inf')
                        is_viral = True
                    else:
                        continue
                else:
                    growth_rate = current_count / previous_count
                    is_viral = growth_rate >= threshold

                if is_viral:
                    viral_topics.append({
                        "keyword": keyword,
                        "current_count": current_count,
                        "previous_count": previous_count,
                        "growth_rate": round(growth_rate, 2) if growth_rate != float('inf') else "æ–°è¯é¢˜",
                        "sample_titles": current_keyword_titles[keyword][:3],
                        "alert_level": "é«˜" if growth_rate > threshold * 2 else "trong"
                    })

            # theoå¢é•¿ç‡sáº¯p xáº¿p
            viral_topics.sort(
                key=lambda x: x["current_count"] if x["growth_rate"] == "æ–°è¯é¢˜" else x["growth_rate"],
                reverse=True
            )

            if not viral_topics:
                return {
                    "success": True,
                    "viral_topics": [],
                    "total_detected": 0,
                    "message": f"æœªæ£€æµ‹Ä‘áº¿nçƒ­åº¦å¢é•¿è¶…è¿‡ {threshold} å€cá»§aè¯é¢˜"
                }

            return {
                "success": True,
                "viral_topics": viral_topics,
                "total_detected": len(viral_topics),
                "threshold": threshold,
                "time_window": time_window,
                "detection_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    def predict_trending_topics(
        self,
        lookahead_hours: int = 6,
        confidence_threshold: float = 0.7
    ) -> Dict:
        """
        è¯é¢˜é¢„æµ‹ - åŸºá»Ÿå†å²æ•°æ®é¢„æµ‹æœªÄ‘áº¿ncÃ³ thá»ƒèƒ½cá»§axu hÆ°á»›ng nÃ³ng

        Args:
            lookahead_hours: é¢„æµ‹æœªÄ‘áº¿nå¤šå°‘å°giá»
            confidence_threshold: ç½®ä¿¡åº¦é˜ˆå€¼

        Returns:
            é¢„æµ‹cá»§aæ½œåŠ›è¯é¢˜åˆ—è¡¨

        Examples:
            ç”¨æˆ·è¯¢é—®ç¤ºä¾‹ï¼š
            - "é¢„æµ‹æ¥dÆ°á»›iÄ‘áº¿n6å°giá»cÃ³ thá»ƒèƒ½cá»§axu hÆ°á»›ng nÃ³ngè¯é¢˜"
            - "cÃ³å“ªäº›è¯é¢˜cÃ³ thá»ƒèƒ½sáº½ç«èµ·Ä‘áº¿n"
            - "æ—©æœŸå‘ç°æ½œåŠ›è¯é¢˜"

            ä»£ç è°ƒç”¨ç¤ºä¾‹ï¼š
            >>> tools = AnalyticsTools()
            >>> result = tools.predict_trending_topics(
            ...     lookahead_hours=6,
            ...     confidence_threshold=0.7
            ... )
            >>> print(result['predicted_topics'])
        """
        try:
            # tham sá»‘éªŒè¯
            lookahead_hours = validate_limit(lookahead_hours, default=6, max_limit=48)

            if not 0 <= confidence_threshold <= 1:
                raise InvalidParameterError(
                    "confidence_threshold å¿…é¡»á»Ÿ 0 Ä‘áº¿n 1 ä¹‹é—´",
                    suggestion="æ¨èå€¼ï¼š0.6-0.8"
                )

            # æ”¶é›†nháº¥tè¿‘3å¤©cá»§adá»¯ liá»‡uç”¨á»Ÿé¢„æµ‹
            keyword_trends = defaultdict(list)

            for days_ago in range(3, 0, -1):
                date = datetime.now() - timedelta(days=days_ago)

                try:
                    all_titles, _, _ = self.data_service.parser.read_all_titles_for_date(
                        date=date
                    )

                    # thá»‘ng kÃªå…³é”®è¯
                    keywords_count = Counter()
                    for _, titles in all_titles.items():
                        for title in titles.keys():
                            keywords = self._extract_keywords(title)
                            keywords_count.update(keywords)

                    # báº£n ghimá»—iå…³é”®è¯cá»§alá»‹ch sá»­dá»¯ liá»‡u
                    for keyword, count in keywords_count.items():
                        keyword_trends[keyword].append(count)

                except DataNotFoundError:
                    pass

            # thÃªmhÃ´m naycá»§adá»¯ liá»‡u
            try:
                all_titles, _, _ = self.data_service.parser.read_all_titles_for_date()

                keywords_count = Counter()
                keyword_titles = defaultdict(list)

                for _, titles in all_titles.items():
                    for title in titles.keys():
                        keywords = self._extract_keywords(title)
                        keywords_count.update(keywords)

                        for kw in keywords:
                            keyword_titles[kw].append(title)

                for keyword, count in keywords_count.items():
                    keyword_trends[keyword].append(count)

            except DataNotFoundError:
                raise DataNotFoundError(
                    "æœªæ‰¾Ä‘áº¿nhÃ´m naycá»§aæ•°æ®",
                    suggestion="è¯·v.v.å¾…çˆ¬è™«ä»»åŠ¡hoÃ n thÃ nh"
                )

            # é¢„æµ‹æ½œåŠ›è¯é¢˜
            predicted_topics = []

            for keyword, trend_data in keyword_trends.items():
                if len(trend_data) < 2:
                    continue

                # ç®€å•cá»§açº¿æ€§è¶‹åŠ¿é¢„æµ‹
                # tÃ­nh toÃ¡nå¢é•¿ç‡
                recent_value = trend_data[-1]
                previous_value = trend_data[-2] if len(trend_data) >= 2 else 0

                if previous_value == 0:
                    if recent_value >= 3:
                        growth_rate = 1.0
                    else:
                        continue
                else:
                    growth_rate = (recent_value - previous_value) / previous_value

                # åˆ¤æ–­lÃ å¦lÃ trÃªnå‡è¶‹åŠ¿
                if growth_rate > 0.3:  # å¢é•¿è¶…è¿‡30%
                    # tÃ­nh toÃ¡nç½®ä¿¡åº¦ï¼ˆåŸºá»Ÿè¶‹åŠ¿cá»§aç¨³å®šæ€§ï¼‰
                    if len(trend_data) >= 3:
                        # æ£€æŸ¥lÃ å¦è¿ç»­å¢é•¿
                        is_consistent = all(
                            trend_data[i] <= trend_data[i+1]
                            for i in range(len(trend_data)-1)
                        )
                        confidence = 0.9 if is_consistent else 0.7
                    else:
                        confidence = 0.6

                    if confidence >= confidence_threshold:
                        predicted_topics.append({
                            "keyword": keyword,
                            "current_count": recent_value,
                            "growth_rate": round(growth_rate * 100, 2),
                            "confidence": round(confidence, 2),
                            "trend_data": trend_data,
                            "prediction": "trÃªnå‡è¶‹åŠ¿ï¼ŒcÃ³ thá»ƒèƒ½æˆvÃ¬xu hÆ°á»›ng nÃ³ng",
                            "sample_titles": keyword_titles.get(keyword, [])[:3]
                        })

            # theoç½®ä¿¡åº¦vÃ å¢é•¿ç‡sáº¯p xáº¿p
            predicted_topics.sort(
                key=lambda x: (x["confidence"], x["growth_rate"]),
                reverse=True
            )

            return {
                "success": True,
                "predicted_topics": predicted_topics[:20],  # è¿”å›TOP 20
                "total_predicted": len(predicted_topics),
                "lookahead_hours": lookahead_hours,
                "confidence_threshold": confidence_threshold,
                "prediction_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "note": "é¢„æµ‹åŸºá»Ÿå†å²è¶‹åŠ¿ï¼Œå®é™…ç»“æœcÃ³ thá»ƒèƒ½cÃ³åå·®"
            }

        except MCPError as e:
            return {
                "success": False,
                "error": e.to_dict()
            }
        except Exception as e:
            return {
                "success": False,
                "error": {
                    "code": "INTERNAL_ERROR",
                    "message": str(e)
                }
            }

    # ==================== è¾…åŠ©phÆ°Æ¡ng thá»©c ====================

    def _extract_keywords(self, title: str, min_length: int = 2) -> List[str]:
        """
        tá»«æ ‡é¢˜trongæå–å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰

        Args:
            title: æ ‡é¢˜æ–‡æœ¬
            min_length: nháº¥tå°å…³é”®è¯é•¿åº¦

        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        # ç§»é™¤URLvÃ ç‰¹æ®Šå­—ç¬¦
        title = re.sub(r'http[s]?://\S+', '', title)
        title = re.sub(r'[^\w\s]', ' ', title)

        # ç®€å•phÃºtè¯ï¼ˆtheoç©ºæ ¼vÃ å¸¸è§phÃºtéš”ç¬¦ï¼‰
        words = re.split(r'[\sï¼Œã€‚ï¼ï¼Ÿã€]+', title)

        # lá»cåœç”¨è¯vÃ çŸ­è¯
        stopwords = {'cá»§a', 'rá»“i', 'á»Ÿ', 'lÃ ', 'tÃ´i', 'cÃ³', 'vÃ ', 'thÃ¬', 'khÃ´ng', 'ngÆ°á»i', 'Ä‘á»u', 'má»™t', 'má»™t', 'trÃªn', 'cÅ©ng', 'ráº¥t', 'Ä‘áº¿n', 'nÃ³i', 'muá»‘n', 'Ä‘i', 'báº¡n', 'sáº½', 'Ä‘ang', 'khÃ´ng cÃ³', 'xem', 'tá»‘t', 'tá»± mÃ¬nh', 'nÃ y'}

        keywords = [
            word.strip() for word in words
            if word.strip() and len(word.strip()) >= min_length and word.strip() not in stopwords
        ]

        return keywords

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬cá»§aç›¸ä¼¼åº¦

        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2

        Returns:
            ç›¸ä¼¼åº¦phÃºtæ•°ï¼ˆ0-1ä¹‹é—´ï¼‰
        """
        # Sá»­ dá»¥ng SequenceMatcher tÃ­nh toÃ¡nç›¸ä¼¼åº¦
        return SequenceMatcher(None, text1, text2).ratio()

    def _find_unique_topics(self, platform_stats: Dict) -> Dict[str, List[str]]:
        """
        æ‰¾å‡ºå„å¹³å°ç‹¬cÃ³cá»§axu hÆ°á»›ng nÃ³ngè¯é¢˜

        Args:
            platform_stats: å¹³å°ç»Ÿè®¡æ•°æ®

        Returns:
            å„å¹³å°ç‹¬cÃ³è¯é¢˜å­—å…¸
        """
        unique_topics = {}

        # Láº¥ymá»—iå¹³å°cá»§aTOPå…³é”®è¯
        platform_keywords = {}
        for platform, stats in platform_stats.items():
            top_keywords = set([kw for kw, _ in stats["top_keywords"].most_common(10)])
            platform_keywords[platform] = top_keywords

        # æ‰¾å‡ºç‹¬cÃ³å…³é”®è¯
        for platform, keywords in platform_keywords.items():
            # æ‰¾å‡ºå…¶ä»–å¹³å°cá»§aæ‰€cÃ³å…³é”®è¯
            other_keywords = set()
            for other_platform, other_kws in platform_keywords.items():
                if other_platform != platform:
                    other_keywords.update(other_kws)

            # æ‰¾å‡ºç‹¬cÃ³cá»§a
            unique = keywords - other_keywords
            if unique:
                unique_topics[platform] = list(unique)[:5]  # nháº¥tå¤š5ä¸ª

        return unique_topics
