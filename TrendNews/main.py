"""
TrendRadar - Fully Independent Refactored Version

This version is COMPLETELY INDEPENDENT from main.py.
All required functions are re-exported through a compatibility layer.
"""

import sys
import os

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

print("=" * 70)
print("TrendRadar - Fully Refactored & Independent Version")
print("=" * 70)
print()

from src.config import VERSION, CONFIG
from src.core import DataFetcher, PushRecordManager
from src.renderers.html_renderer import HTMLRenderer
from src.processors import (
    save_titles_to_file,
    load_frequency_words,
    read_all_today_titles,
    detect_latest_new_titles,
    count_word_frequency,
    matches_word_groups,
)
from src.utils import (
    get_beijing_time,
    ensure_directory_exists,
    format_time_display,
    is_first_crawl_today,
)

from src.notifiers import send_to_notifications
from src.utils.version_check import check_version_update

# Import Vietnam fetcher for scraper-type platforms
try:
    from src.core.vietnam_fetcher import VietnamDataFetcher
    VIETNAM_SCRAPER_AVAILABLE = True
except ImportError:
    VIETNAM_SCRAPER_AVAILABLE = False
    print("‚ö† VietnamDataFetcher kh√¥ng kh·∫£ d·ª•ng (b·ªè qua scraper sources). Ch·∫°y: pip install beautifulsoup4 lxml")

from pathlib import Path
from typing import Dict, List, Optional, Tuple
import webbrowser


class NewsAnalyzer:
    """
    Fully refactored NewsAnalyzer using modular structure.
    
    Uses refactored modules where available, with legacy functions
    for complex rendering/notification logic (temporary).
    """

    MODE_STRATEGIES = {
        "incremental": {
            "mode_name": "Ch·∫ø ƒë·ªô tƒÉng d·∫ßn",
            "description": "Ch·∫ø ƒë·ªô tƒÉng d·∫ßnÔºàch·ªâ quan t√¢m tin t·ª©c m·ªõiÔºåÊó†m·ªõigi·ªùkh√¥ngÊé®ÈÄÅÔºâ",
            "realtime_report_type": "ÂÆûgi·ªùtƒÉng d·∫ßn",
            "summary_report_type": "ÂΩìng√†yt·ªïng h·ª£p",
            "should_send_realtime": True,
            "should_generate_summary": True,
            "summary_mode": "daily",
        },
        "current": {
            "mode_name": "b·∫£ng x·∫øp h·∫°ng hi·ªán t·∫°ich·∫ø ƒë·ªô",
            "description": "b·∫£ng x·∫øp h·∫°ng hi·ªán t·∫°ich·∫ø ƒë·ªôÔºàtin t·ª©c kh·ªõp b·∫£ng x·∫øp h·∫°ng hi·ªán t·∫°i + m·ªõitin t·ª©ckhu v·ª±c + theogi·ªùÊé®ÈÄÅÔºâ",
            "realtime_report_type": "ÂÆûgi·ªùb·∫£ng x·∫øp h·∫°ng hi·ªán t·∫°i",
            "summary_report_type": "b·∫£ng x·∫øp h·∫°ng hi·ªán t·∫°it·ªïng h·ª£p",
            "should_send_realtime": True,
            "should_generate_summary": True,
            "summary_mode": "current",
        },
        "daily": {
            "mode_name": "ÂΩìng√†yt·ªïng h·ª£pch·∫ø ƒë·ªô",
            "description": "ÂΩìng√†yt·ªïng h·ª£pch·∫ø ƒë·ªôÔºàÊâÄc√≥kh·ªõptin t·ª©c + m·ªõitin t·ª©ckhu v·ª±c + theogi·ªùÊé®ÈÄÅÔºâ",
            "realtime_report_type": "",
            "summary_report_type": "ÂΩìng√†yt·ªïng h·ª£p",
            "should_send_realtime": False,
            "should_generate_summary": True,
            "summary_mode": "daily",
        },
    }

    def __init__(self):
        self.request_interval = CONFIG["REQUEST_INTERVAL"]
        self.report_mode = CONFIG["REPORT_MODE"]
        self.rank_threshold = CONFIG["RANK_THRESHOLD"]
        self.is_github_actions = os.environ.get("GITHUB_ACTIONS") == "true"
        self.is_docker_container = self._detect_docker_environment()
        self.update_info = None
        self.proxy_url = None
        self._setup_proxy()
        self.data_fetcher = DataFetcher(self.proxy_url)
        
        # Initialize Vietnam fetcher for scraper-type platforms
        self.vn_fetcher = None
        scraper_platforms = [p for p in CONFIG["PLATFORMS"] if p.get("type") == "scraper"]
        if VIETNAM_SCRAPER_AVAILABLE and scraper_platforms:
            self.vn_fetcher = VietnamDataFetcher()

        if self.is_github_actions:
            self._check_version_update()

    def _detect_docker_environment(self) -> bool:
        try:
            if os.environ.get("DOCKER_CONTAINER") == "true":
                return True
            if os.path.exists("/.dockerenv"):
                return True
            return False
        except Exception:
            return False

    def _should_open_browser(self) -> bool:
        return not self.is_github_actions and not self.is_docker_container

    def _setup_proxy(self) -> None:
        if not self.is_github_actions and CONFIG["USE_PROXY"]:
            self.proxy_url = CONFIG["DEFAULT_PROXY"]
            print("M√¥i tr∆∞·ªùng c·ª•c b·ªôÔºås·ª≠ d·ª•ng proxy")
        elif not self.is_github_actions and not CONFIG["USE_PROXY"]:
            print("M√¥i tr∆∞·ªùng c·ª•c b·ªôÔºåch∆∞a b·∫≠t proxy")
        else:
            print("GitHub Actionsm√¥i tr∆∞·ªùngÔºåkh√¥ngs·ª≠ d·ª•ng proxy")

    def _check_version_update(self) -> None:
        try:
            need_update, remote_version = check_version_update(
                VERSION, CONFIG["VERSION_CHECK_URL"], self.proxy_url
            )

            if need_update and remote_version:
                self.update_info = {
                    "current_version": VERSION,
                    "remote_version": remote_version,
                }
                print(f"ÂèëÁé∞m·ªõiÁâàÊú¨: {remote_version} (hi·ªán t·∫°i: {VERSION})")
            else:
                print("ÁâàÊú¨Ê£ÄÊü•ho√†n th√†nhÔºåhi·ªán t·∫°iv√¨m·ªõi nh·∫•tÁâàÊú¨")
        except Exception as e:
            print(f"Ki·ªÉm tra phi√™n b·∫£n l·ªói: {e}")

    def _get_mode_strategy(self) -> Dict:
        return self.MODE_STRATEGIES.get(self.report_mode, self.MODE_STRATEGIES["daily"])

    def _has_notification_configured(self) -> bool:
        return any([
            (CONFIG["TELEGRAM_BOT_TOKEN"] and CONFIG["TELEGRAM_CHAT_ID"]),
            (CONFIG["EMAIL_FROM"] and CONFIG["EMAIL_PASSWORD"] and CONFIG["EMAIL_TO"]),
        ])

    def _has_valid_content(self, stats: List[Dict], new_titles: Optional[Dict] = None) -> bool:
        if self.report_mode in ["incremental", "current"]:
            return any(stat["count"] > 0 for stat in stats)
        else:
            has_matched_news = any(stat["count"] > 0 for stat in stats)
            has_new_news = bool(new_titles and any(len(titles) > 0 for titles in new_titles.values()))
            return has_matched_news or has_new_news

    def _load_analysis_data(self) -> Optional[Tuple]:
        try:
            # Get ALL platform IDs from unified PLATFORMS config
            current_platform_ids = [platform["id"] for platform in CONFIG["PLATFORMS"]]
            print(f"hi·ªán t·∫°iÁõëÊéßÂπ≥Âè∞: {current_platform_ids}")

            all_results, id_to_name, title_info = read_all_today_titles(current_platform_ids)

            if not all_results:
                print("kh√¥ng c√≥Êâæƒë·∫øntrong ng√†yc·ªßad·ªØ li·ªáu")
                return None

            total_titles = sum(len(titles) for titles in all_results.values())
            print(f"ƒë·ªçcƒë·∫øn {total_titles} ti√™u ƒë·ªÅÔºàƒë√£theohi·ªán t·∫°iÁõëÊéßÂπ≥Âè∞l·ªçcÔºâ")

            new_titles = detect_latest_new_titles(current_platform_ids)
            word_groups, filter_words = load_frequency_words()

            return (all_results, id_to_name, title_info, new_titles, word_groups, filter_words)
        except Exception as e:
            print(f"d·ªØ li·ªáut·∫£ith·∫•t b·∫°i: {e}")
            return None

    def _prepare_current_title_info(self, results: Dict, time_info: str) -> Dict:
        title_info = {}
        for source_id, titles_data in results.items():
            title_info[source_id] = {}
            for title, title_data in titles_data.items():
                ranks = title_data.get("ranks", [])
                url = title_data.get("url", "")
                mobile_url = title_data.get("mobileUrl", "")

                title_info[source_id][title] = {
                    "first_time": time_info,
                    "last_time": time_info,
                    "count": 1,
                    "ranks": ranks,
                    "url": url,
                    "mobileUrl": mobile_url,
                }
        return title_info

    def _run_analysis_pipeline(
        self, data_source, mode, title_info, new_titles, word_groups,
        filter_words, id_to_name, failed_ids=None, is_daily_summary=False
    ) -> Tuple:
        stats, total_titles = count_word_frequency(
            data_source, word_groups, filter_words, id_to_name,
            title_info, self.rank_threshold, new_titles, mode=mode
        )

        html_file = HTMLRenderer.generate_report(
            stats, total_titles, failed_ids=failed_ids, new_titles=new_titles,
            id_to_name=id_to_name, mode=mode, is_daily_summary=is_daily_summary,
            update_info=self.update_info if CONFIG["SHOW_VERSION_UPDATE"] else None
        )

        return stats, html_file

    def _send_notification_if_needed(
        self, stats, report_type, mode, failed_ids=None,
        new_titles=None, id_to_name=None, html_file_path=None
    ) -> bool:
        has_notification = self._has_notification_configured()

        if CONFIG["ENABLE_NOTIFICATION"] and has_notification and self._has_valid_content(stats, new_titles):
            send_to_notifications(
                stats, failed_ids or [], report_type, new_titles, id_to_name,
                self.update_info, self.proxy_url, mode=mode, html_file_path=html_file_path
            )
            return True
        elif CONFIG["ENABLE_NOTIFICATION"] and not has_notification:
            print("‚ö†Ô∏è c·∫£nh b√°oÔºöth√¥ng b√°oÂäüËÉΩƒë√£ÂêØÁî®nh∆∞ngCh∆∞a c·∫•u h√¨nh k√™nh th√¥ng b√°o n√†oÔºås·∫Ωb·ªè qua g·ª≠i th√¥ng b√°o")
        elif not CONFIG["ENABLE_NOTIFICATION"]:
            print(f"b·ªè qua{report_type}th√¥ng b√°oÔºöth√¥ng b√°oÂäüËÉΩƒë√£Á¶ÅÁî®")
        elif CONFIG["ENABLE_NOTIFICATION"] and has_notification and not self._has_valid_content(stats, new_titles):
            mode_strategy = self._get_mode_strategy()
            if "ÂÆûgi·ªù" in report_type:
                print(f"B·ªè quaÂÆûgi·ªùÊé®ÈÄÅth√¥ng b√°oÔºö{mode_strategy['mode_name']}d∆∞·ªõiÊú™Ê£ÄÊµãƒë·∫ønkh·ªõpc·ªßatin t·ª©c")
            else:
                print(f"B·ªè qua{mode_strategy['summary_report_type']}th√¥ng b√°oÔºöÊú™kh·ªõpƒë·∫ønc√≥Êïàc·ªßatin t·ª©cn·ªôi dung")

        return False

    def _generate_summary_report(self, mode_strategy: Dict) -> Optional[str]:
        summary_type = "b·∫£ng x·∫øp h·∫°ng hi·ªán t·∫°it·ªïng h·ª£p" if mode_strategy["summary_mode"] == "current" else "ÂΩìng√†yt·ªïng h·ª£p"
        print(f"t·∫°o{summary_type}b√°o c√°o...")

        analysis_data = self._load_analysis_data()
        if not analysis_data:
            return None

        all_results, id_to_name, title_info, new_titles, word_groups, filter_words = analysis_data

        stats, html_file = self._run_analysis_pipeline(
            all_results, mode_strategy["summary_mode"], title_info, new_titles,
            word_groups, filter_words, id_to_name, is_daily_summary=True
        )

        print(f"{summary_type}b√°o c√°oƒë√£t·∫°o: {html_file}")

        self._send_notification_if_needed(
            stats, mode_strategy["summary_report_type"], mode_strategy["summary_mode"],
            failed_ids=[], new_titles=new_titles, id_to_name=id_to_name, html_file_path=html_file
        )

        return html_file

    def _generate_summary_html(self, mode: str = "daily") -> Optional[str]:
        summary_type = "b·∫£ng x·∫øp h·∫°ng hi·ªán t·∫°it·ªïng h·ª£p" if mode == "current" else "ÂΩìng√†yt·ªïng h·ª£p"
        print(f"t·∫°o{summary_type}HTML...")

        analysis_data = self._load_analysis_data()
        if not analysis_data:
            return None

        all_results, id_to_name, title_info, new_titles, word_groups, filter_words = analysis_data

        _, html_file = self._run_analysis_pipeline(
            all_results, mode, title_info, new_titles, word_groups,
            filter_words, id_to_name, is_daily_summary=True
        )

        print(f"{summary_type}HTMLƒë√£t·∫°o: {html_file}")
        return html_file

    def _initialize_and_check_config(self) -> None:
        now = get_beijing_time()
        print(f"hi·ªán t·∫°iÂåó‰∫¨th·ªùi gian: {now.strftime('%Y-%m-%d %H:%M:%S')}")

        if not CONFIG["ENABLE_CRAWLER"]:
            print("Áà¨Ëô´ÂäüËÉΩƒë√£Á¶ÅÁî®ÔºàENABLE_CRAWLER=FalseÔºâÔºåCh∆∞∆°ng tr√¨nh tho√°t")
            return

        has_notification = self._has_notification_configured()
        if not CONFIG["ENABLE_NOTIFICATION"]:
            print("th√¥ng b√°oÂäüËÉΩƒë√£Á¶ÅÁî®ÔºàENABLE_NOTIFICATION=FalseÔºâÔºås·∫ΩÂè™ËøõË°åd·ªØ li·ªáuÊäìÂèñ")
        elif not has_notification:
            print("Ch∆∞a c·∫•u h√¨nh k√™nh th√¥ng b√°o n√†oÔºås·∫ΩÂè™ËøõË°åd·ªØ li·ªáuÊäìÂèñÔºåkh√¥ngg·ª≠ith√¥ng b√°o")
        else:
            print("th√¥ng b√°oÂäüËÉΩƒë√£ÂêØÁî®Ôºås·∫Ωg·ª≠ith√¥ng b√°o")

        mode_strategy = self._get_mode_strategy()
        print(f"b√°o c√°oCh·∫ø ƒë·ªô: {self.report_mode}")
        print(f"ËøêË°åCh·∫ø ƒë·ªô: {mode_strategy['description']}")

    def _crawl_data(self) -> Tuple:
        # Separate platforms by type
        api_platforms = []
        scraper_platforms = []
        
        for platform in CONFIG["PLATFORMS"]:
            if platform.get("type") == "scraper":
                scraper_platforms.append(platform)
            else:
                api_platforms.append(platform)

        # Build ids for API platforms
        api_ids = []
        for platform in api_platforms:
            if "name" in platform:
                api_ids.append((platform["id"], platform["name"]))
            else:
                api_ids.append(platform["id"])

        all_platform_names = [p.get('name', p['id']) for p in CONFIG['PLATFORMS']]
        print(f"ÈÖçÁΩÆc·ªßaÁõëÊéßÂπ≥Âè∞: {all_platform_names}")
        print(f"b·∫Øt ƒë·∫ßuthu th·∫≠pd·ªØ li·ªáuÔºåY√™u c·∫ßuÈó¥Èöî {self.request_interval} mili gi√¢y")
        ensure_directory_exists("output")

        # Crawl API sources
        results, id_to_name, failed_ids = self.data_fetcher.crawl_websites(api_ids, self.request_interval)

        # Crawl scraper sources (Vietnamese)
        if self.vn_fetcher and scraper_platforms:
            scraper_ids = []
            for platform in scraper_platforms:
                if "name" in platform:
                    scraper_ids.append((platform["id"], platform["name"]))
                else:
                    scraper_ids.append(platform["id"])
            
            if scraper_ids:
                print(f"\nüáªüá≥ Scraper sources: {[p.get('name', p['id']) for p in scraper_platforms]}")
                vn_results, vn_id_to_name, vn_failed_ids = self.vn_fetcher.crawl_websites(scraper_ids)
                
                # Merge results
                results.update(vn_results)
                id_to_name.update(vn_id_to_name)
                failed_ids.extend(vn_failed_ids)

        title_file = save_titles_to_file(results, id_to_name, failed_ids)
        print(f"Ê†áÈ¢òƒë√£l∆∞uƒë·∫øn: {title_file}")

        return results, id_to_name, failed_ids

    def _execute_mode_strategy(self, mode_strategy, results, id_to_name, failed_ids) -> Optional[str]:
        # Get all platform IDs from unified PLATFORMS config
        current_platform_ids = [platform["id"] for platform in CONFIG["PLATFORMS"]]

        new_titles = detect_latest_new_titles(current_platform_ids)
        time_info = Path(save_titles_to_file(results, id_to_name, failed_ids)).stem
        word_groups, filter_words = load_frequency_words()

        if self.report_mode == "current":
            analysis_data = self._load_analysis_data()
            if analysis_data:
                all_results, historical_id_to_name, historical_title_info, historical_new_titles, _, _ = analysis_data

                print(f"currentch·∫ø ƒë·ªôÔºöS·ª≠ d·ª•ngl·ªçcÂêéc·ªßaÂéÜÂè≤Êï∞ÊçÆÔºåbao g·ªìm n·ªÅn t·∫£ngÔºö{list(all_results.keys())}")

                stats, html_file = self._run_analysis_pipeline(
                    all_results, self.report_mode, historical_title_info, historical_new_titles,
                    word_groups, filter_words, historical_id_to_name, failed_ids=failed_ids
                )

                combined_id_to_name = {**historical_id_to_name, **id_to_name}

                print(f"HTMLb√°o c√°oƒë√£t·∫°o: {html_file}")

                summary_html = None
                if mode_strategy["should_send_realtime"]:
                    self._send_notification_if_needed(
                        stats, mode_strategy["realtime_report_type"], self.report_mode,
                        failed_ids=failed_ids, new_titles=historical_new_titles,
                        id_to_name=combined_id_to_name, html_file_path=html_file
                    )
            else:
                print("‚ùå ‰∏•Èáçl·ªóiÔºöÊó†Ê≥ïƒë·ªçcÂàöl∆∞uc·ªßad·ªØ li·ªáufile")
                raise RuntimeError("Êï∞ÊçÆm·ªôtËá¥ÊÄßÊ£ÄÊü•Â§±Ë¥•Ôºöƒê·ªçc ngay sau khi l∆∞u th·∫•t b·∫°i")
        else:
            title_info = self._prepare_current_title_info(results, time_info)
            stats, html_file = self._run_analysis_pipeline(
                results, self.report_mode, title_info, new_titles,
                word_groups, filter_words, id_to_name, failed_ids=failed_ids
            )
            print(f"HTMLb√°o c√°oƒë√£t·∫°o: {html_file}")

            summary_html = None
            if mode_strategy["should_send_realtime"]:
                self._send_notification_if_needed(
                    stats, mode_strategy["realtime_report_type"], self.report_mode,
                    failed_ids=failed_ids, new_titles=new_titles,
                    id_to_name=id_to_name, html_file_path=html_file
                )

        summary_html = None
        if mode_strategy["should_generate_summary"]:
            if mode_strategy["should_send_realtime"]:
                summary_html = self._generate_summary_html(mode_strategy["summary_mode"])
            else:
                summary_html = self._generate_summary_report(mode_strategy)

        if self._should_open_browser() and html_file:
            if summary_html:
                summary_url = "file://" + str(Path(summary_html).resolve())
                print(f"Ê≠£·ªüm·ªüt·ªïng h·ª£pb√°o c√°o: {summary_url}")
                webbrowser.open(summary_url)
            else:
                file_url = "file://" + str(Path(html_file).resolve())
                print(f"Ê≠£·ªüm·ªüHTMLb√°o c√°o: {file_url}")
                webbrowser.open(file_url)
        elif self.is_docker_container and html_file:
            if summary_html:
                print(f"t·ªïng h·ª£pb√°o c√°oƒë√£t·∫°oÔºàDockerm√¥i tr∆∞·ªùngÔºâ: {summary_html}")
            else:
                print(f"HTMLb√°o c√°oƒë√£t·∫°oÔºàDockerm√¥i tr∆∞·ªùngÔºâ: {html_file}")

        return summary_html

    def run(self) -> None:
        try:
            self._initialize_and_check_config()
            mode_strategy = self._get_mode_strategy()
            results, id_to_name, failed_ids = self._crawl_data()
            self._execute_mode_strategy(mode_strategy, results, id_to_name, failed_ids)
        except Exception as e:
            print(f"ph√¢n t√≠chÊµÅÁ®ãÊâßË°åÂá∫Èîô: {e}")
            raise


def main():
    try:
        print("Using refactored modular structure with legacy function compatibility")
        print()
        analyzer = NewsAnalyzer()
        analyzer.run()
        return True
    except FileNotFoundError as e:
        print(f"‚ùå File c·∫•u h√¨nhl·ªói: {e}")
        print("\nVui l√≤ng ƒë·∫£m b·∫£o c√°c file sau t·ªìn t·∫°i:")
        print("  ‚Ä¢ config/config.yaml")
        print("  ‚Ä¢ config/frequency_words.txt")
        return False
    except Exception as e:
        print(f"‚ùå L·ªói ch·∫°y ch∆∞∆°ng tr√¨nh: {e}")
        raise


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
